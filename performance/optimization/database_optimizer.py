#!/usr/bin/env python3
"""
HankookTire SmartSensor 2.0 - Database Performance Optimizer
ì°¨ì„¸ëŒ€ í†µí•© ìŠ¤ë§ˆíŠ¸ íƒ€ì´ì–´ ì„¼ì„œ ì‹œìŠ¤í…œ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™” ë„êµ¬
- ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™”
- ì¸ë±ìŠ¤ ìë™ ìƒì„± ë° ê´€ë¦¬
- íŒŒí‹°ì…”ë‹ ì „ëµ êµ¬í˜„
- ì—°ê²° í’€ ìµœì í™”
- ìºì‹± ì „ëµ êµ¬í˜„
- ë°ì´í„° ì•„ì¹´ì´ë¹™
"""

import asyncio
import psycopg2
from psycopg2.extras import RealDictCursor
import redis
import json
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from enum import Enum
import statistics
import threading
from concurrent.futures import ThreadPoolExecutor
import os

# ì„¤ì •
POSTGRES_HOST = os.getenv('POSTGRES_HOST', 'localhost')
POSTGRES_PORT = int(os.getenv('POSTGRES_PORT', '5432'))
POSTGRES_DB = os.getenv('POSTGRES_DB', 'smarttire_sensors')
POSTGRES_USER = os.getenv('POSTGRES_USER', 'smarttire')
POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD', 'password')

REDIS_HOST = os.getenv('REDIS_HOST', 'localhost')
REDIS_PORT = int(os.getenv('REDIS_PORT', '6379'))
REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', '')

# ìµœì í™” ì„¤ì •
SLOW_QUERY_THRESHOLD = float(os.getenv('SLOW_QUERY_THRESHOLD', '1.0'))  # 1ì´ˆ
INDEX_USAGE_THRESHOLD = float(os.getenv('INDEX_USAGE_THRESHOLD', '0.1'))  # 10%
CACHE_TTL_SECONDS = int(os.getenv('CACHE_TTL_SECONDS', '300'))  # 5ë¶„
PARTITION_SIZE_MB = int(os.getenv('PARTITION_SIZE_MB', '1000'))  # 1GB

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OptimizationType(Enum):
    """ìµœì í™” íƒ€ì…"""
    INDEX_CREATION = "index_creation"
    QUERY_OPTIMIZATION = "query_optimization"
    PARTITIONING = "partitioning"
    CACHING = "caching"
    CONNECTION_POOLING = "connection_pooling"
    ARCHIVING = "archiving"
    VACUUM_ANALYZE = "vacuum_analyze"

class QueryType(Enum):
    """ì¿¼ë¦¬ íƒ€ì…"""
    SELECT = "select"
    INSERT = "insert"
    UPDATE = "update"
    DELETE = "delete"
    AGGREGATE = "aggregate"

@dataclass
class QueryAnalysis:
    """ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼"""
    query_text: str
    query_type: QueryType
    execution_time_ms: float
    rows_examined: int
    rows_returned: int
    index_usage: List[str]
    table_scans: int
    cpu_cost: float
    io_cost: float
    optimization_suggestions: List[str]
    timestamp: datetime

@dataclass
class IndexRecommendation:
    """ì¸ë±ìŠ¤ ì¶”ì²œ"""
    table_name: str
    column_names: List[str]
    index_type: str  # btree, hash, gin, gist
    expected_improvement: float
    usage_frequency: int
    size_estimate_mb: float
    creation_time_estimate: int

@dataclass
class PartitionStrategy:
    """íŒŒí‹°ì…”ë‹ ì „ëµ"""
    table_name: str
    partition_key: str
    partition_type: str  # range, list, hash
    partition_interval: str
    retention_period: int
    expected_performance_gain: float

class DatabaseOptimizer:
    """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ê´€ë¦¬ì"""
    
    def __init__(self):
        self.redis_client = None
        self.connection_pool = None
        self.query_stats = {}
        self.optimization_history = []
        
    async def initialize(self):
        """ì´ˆê¸°í™”"""
        # Redis ì—°ê²°
        self.redis_client = redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            password=REDIS_PASSWORD if REDIS_PASSWORD else None,
            decode_responses=True
        )
        
        # ì—°ê²° í’€ ì´ˆê¸°í™”
        self.connection_pool = psycopg2.pool.ThreadedConnectionPool(
            5, 20,  # min, max connections
            host=POSTGRES_HOST,
            port=POSTGRES_PORT,
            database=POSTGRES_DB,
            user=POSTGRES_USER,
            password=POSTGRES_PASSWORD
        )
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í…Œì´ë¸” ìƒì„±
        await self.create_performance_tables()
        
        # ê¸°ë³¸ ìµœì í™” ì‹¤í–‰
        await self.apply_baseline_optimizations()
        
        logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")

    async def create_performance_tables(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í…Œì´ë¸” ìƒì„±"""
        conn = self.get_connection()
        
        try:
            with conn.cursor() as cursor:
                # ì¿¼ë¦¬ ì„±ëŠ¥ ë¡œê·¸ í…Œì´ë¸”
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS performance.query_performance (
                        id SERIAL PRIMARY KEY,
                        query_hash VARCHAR(64) NOT NULL,
                        query_text TEXT NOT NULL,
                        query_type VARCHAR(20) NOT NULL,
                        execution_time_ms FLOAT NOT NULL,
                        rows_examined INTEGER,
                        rows_returned INTEGER,
                        index_usage JSONB,
                        table_scans INTEGER DEFAULT 0,
                        cpu_cost FLOAT,
                        io_cost FLOAT,
                        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ í…Œì´ë¸”
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS performance.index_usage_stats (
                        id SERIAL PRIMARY KEY,
                        schema_name VARCHAR(64) NOT NULL,
                        table_name VARCHAR(64) NOT NULL,
                        index_name VARCHAR(64) NOT NULL,
                        scans INTEGER DEFAULT 0,
                        tuples_read INTEGER DEFAULT 0,
                        tuples_fetched INTEGER DEFAULT 0,
                        last_used TIMESTAMP,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(schema_name, table_name, index_name)
                    )
                """)
                
                # í…Œì´ë¸” í¬ê¸° í†µê³„ í…Œì´ë¸”
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS performance.table_size_stats (
                        id SERIAL PRIMARY KEY,
                        schema_name VARCHAR(64) NOT NULL,
                        table_name VARCHAR(64) NOT NULL,
                        size_bytes BIGINT NOT NULL,
                        row_count BIGINT,
                        avg_row_size INTEGER,
                        last_vacuum TIMESTAMP,
                        last_analyze TIMESTAMP,
                        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # ìµœì í™” ì´ë ¥ í…Œì´ë¸”
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS performance.optimization_history (
                        id SERIAL PRIMARY KEY,
                        optimization_type VARCHAR(30) NOT NULL,
                        target_object VARCHAR(100) NOT NULL,
                        action_taken TEXT NOT NULL,
                        before_metrics JSONB,
                        after_metrics JSONB,
                        performance_gain FLOAT,
                        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # ì¸ë±ìŠ¤ ìƒì„±
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_query_performance_hash ON performance.query_performance(query_hash)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_query_performance_timestamp ON performance.query_performance(timestamp)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_index_usage_table ON performance.index_usage_stats(schema_name, table_name)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_table_size_stats_table ON performance.table_size_stats(schema_name, table_name)")
                
            conn.commit()
            logger.info("âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            conn.rollback()
            raise
        finally:
            self.return_connection(conn)

    def get_connection(self):
        """ì—°ê²° í’€ì—ì„œ ì—°ê²° íšë“"""
        return self.connection_pool.getconn()
        
    def return_connection(self, conn):
        """ì—°ê²° í’€ì— ì—°ê²° ë°˜í™˜"""
        self.connection_pool.putconn(conn)

    async def apply_baseline_optimizations(self):
        """ê¸°ë³¸ ìµœì í™” ì ìš©"""
        logger.info("ğŸ”§ ê¸°ë³¸ ìµœì í™” ì ìš© ì¤‘...")
        
        conn = self.get_connection()
        
        try:
            with conn.cursor() as cursor:
                # PostgreSQL ì„¤ì • ìµœì í™”
                optimizations = [
                    # ë©”ëª¨ë¦¬ ì„¤ì •
                    "SET shared_buffers = '256MB'",
                    "SET effective_cache_size = '1GB'",
                    "SET work_mem = '16MB'",
                    "SET maintenance_work_mem = '64MB'",
                    
                    # ì—°ê²° ì„¤ì •
                    "SET max_connections = 200",
                    "SET tcp_keepalives_idle = 600",
                    "SET tcp_keepalives_interval = 30",
                    
                    # ì“°ê¸° ì„±ëŠ¥ ìµœì í™”
                    "SET wal_buffers = '16MB'",
                    "SET checkpoint_completion_target = 0.9",
                    "SET checkpoint_timeout = '10min'",
                    
                    # ì¿¼ë¦¬ ì„±ëŠ¥ ìµœì í™”
                    "SET random_page_cost = 1.1",
                    "SET cpu_tuple_cost = 0.01",
                    "SET cpu_index_tuple_cost = 0.005",
                    
                    # í†µê³„ ìˆ˜ì§‘ í™œì„±í™”
                    "SET track_activities = on",
                    "SET track_counts = on",
                    "SET track_io_timing = on",
                    "SET track_functions = 'all'"
                ]
                
                for optimization in optimizations:
                    try:
                        cursor.execute(optimization)
                        logger.debug(f"âœ… ì ìš©: {optimization}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ ì„¤ì • ì ìš© ì‹¤íŒ¨: {optimization} - {str(e)}")
                
            conn.commit()
            
            # ê¸°ë³¸ ì¸ë±ìŠ¤ ìƒì„±
            await self.create_essential_indexes()
            
            logger.info("âœ… ê¸°ë³¸ ìµœì í™” ì ìš© ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ê¸°ë³¸ ìµœì í™” ì ìš© ì‹¤íŒ¨: {str(e)}")
            conn.rollback()
        finally:
            self.return_connection(conn)

    async def create_essential_indexes(self):
        """í•„ìˆ˜ ì¸ë±ìŠ¤ ìƒì„±"""
        logger.info("ğŸ“Š í•„ìˆ˜ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        essential_indexes = [
            # ì„¼ì„œ ë°ì´í„° ì¸ë±ìŠ¤
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_sensor_readings_timestamp ON sensors.sensor_readings(created_at)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_sensor_readings_device_id ON sensors.sensor_readings(device_id)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_sensor_readings_vehicle_timestamp ON sensors.sensor_readings(vehicle_id, created_at)",
            
            # TPMS ë°ì´í„° ì¸ë±ìŠ¤
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_tpms_data_timestamp ON sensors.tpms_data(created_at)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_tpms_data_vehicle_tire ON sensors.tpms_data(vehicle_id, tire_position)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_tpms_data_pressure ON sensors.tpms_data(pressure_kpa) WHERE pressure_kpa < 200",
            
            # ì°¨ëŸ‰ ë°ì´í„° ì¸ë±ìŠ¤
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_vehicles_status ON sensors.vehicles(status)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_vehicles_location ON sensors.vehicles USING GIST(location)",
            
            # ì•Œë¦¼ ë°ì´í„° ì¸ë±ìŠ¤
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_alerts_timestamp ON alerts.alert_events(created_at)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_alerts_severity ON alerts.alert_events(severity)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_alerts_vehicle_id ON alerts.alert_events(vehicle_id)",
            
            # ì‚¬ìš©ì ì¸ì¦ ì¸ë±ìŠ¤
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_email ON auth.users(email)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_security_events_timestamp ON auth.security_events(timestamp)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_security_events_source_ip ON auth.security_events(source_ip)",
            
            # ë³µí•© ì¸ë±ìŠ¤
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_sensor_readings_composite ON sensors.sensor_readings(device_id, created_at, sensor_type)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_tpms_alerts_composite ON sensors.tpms_data(vehicle_id, created_at) WHERE pressure_kpa < 200 OR temperature_celsius > 80"
        ]
        
        conn = self.get_connection()
        
        try:
            with conn.cursor() as cursor:
                for index_sql in essential_indexes:
                    try:
                        start_time = time.time()
                        cursor.execute(index_sql)
                        execution_time = time.time() - start_time
                        
                        index_name = index_sql.split('IF NOT EXISTS ')[1].split(' ON')[0]
                        logger.info(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {index_name} ({execution_time:.2f}ì´ˆ)")
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
                
            conn.commit()
            
        except Exception as e:
            logger.error(f"âŒ í•„ìˆ˜ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            conn.rollback()
        finally:
            self.return_connection(conn)

    async def analyze_query_performance(self, days: int = 7) -> List[QueryAnalysis]:
        """ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„"""
        logger.info(f"ğŸ” ìµœê·¼ {days}ì¼ê°„ ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
        
        conn = self.get_connection()
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # ëŠë¦° ì¿¼ë¦¬ ì¡°íšŒ
                cursor.execute("""
                    SELECT 
                        query,
                        calls,
                        total_time,
                        mean_time,
                        max_time,
                        min_time,
                        rows,
                        shared_blks_hit,
                        shared_blks_read,
                        shared_blks_dirtied,
                        temp_blks_read,
                        temp_blks_written
                    FROM pg_stat_statements 
                    WHERE mean_time > %s
                    ORDER BY total_time DESC
                    LIMIT 50
                """, (SLOW_QUERY_THRESHOLD * 1000,))  # ë°€ë¦¬ì´ˆ ë³€í™˜
                
                slow_queries = cursor.fetchall()
                
                analyses = []
                for query_stat in slow_queries:
                    analysis = await self._analyze_single_query(query_stat)
                    analyses.append(analysis)
                
                # ë¶„ì„ ê²°ê³¼ ì €ì¥
                await self._save_query_analyses(analyses)
                
                logger.info(f"âœ… {len(analyses)}ê°œ ì¿¼ë¦¬ ë¶„ì„ ì™„ë£Œ")
                return analyses
                
        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return []
        finally:
            self.return_connection(conn)

    async def _analyze_single_query(self, query_stat: Dict) -> QueryAnalysis:
        """ë‹¨ì¼ ì¿¼ë¦¬ ë¶„ì„"""
        query_text = query_stat['query']
        
        # ì¿¼ë¦¬ íƒ€ì… ê°ì§€
        query_type = QueryType.SELECT
        if query_text.upper().strip().startswith('INSERT'):
            query_type = QueryType.INSERT
        elif query_text.upper().strip().startswith('UPDATE'):
            query_type = QueryType.UPDATE
        elif query_text.upper().strip().startswith('DELETE'):
            query_type = QueryType.DELETE
        elif any(keyword in query_text.upper() for keyword in ['COUNT', 'SUM', 'AVG', 'GROUP BY']):
            query_type = QueryType.AGGREGATE
        
        # ìµœì í™” ì œì•ˆ ìƒì„±
        suggestions = await self._generate_optimization_suggestions(query_stat)
        
        return QueryAnalysis(
            query_text=query_text,
            query_type=query_type,
            execution_time_ms=query_stat['mean_time'],
            rows_examined=query_stat.get('rows', 0),
            rows_returned=query_stat.get('rows', 0),
            index_usage=[],  # pg_stat_statementsì—ì„œëŠ” ì§ì ‘ ì œê³µë˜ì§€ ì•ŠìŒ
            table_scans=query_stat.get('shared_blks_read', 0),
            cpu_cost=query_stat['total_time'],
            io_cost=query_stat.get('shared_blks_read', 0) * 0.1,
            optimization_suggestions=suggestions,
            timestamp=datetime.utcnow()
        )

    async def _generate_optimization_suggestions(self, query_stat: Dict) -> List[str]:
        """ìµœì í™” ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        query_text = query_stat['query'].upper()
        
        # ì¸ë±ìŠ¤ ì œì•ˆ
        if query_stat.get('shared_blks_read', 0) > 1000:
            suggestions.append("ë†’ì€ ë””ìŠ¤í¬ I/O ê°ì§€ - ì ì ˆí•œ ì¸ë±ìŠ¤ ìƒì„± ê³ ë ¤")
        
        if 'WHERE' in query_text and 'INDEX' not in query_text:
            suggestions.append("WHERE ì ˆì˜ ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ ìƒì„± ê³ ë ¤")
        
        if 'ORDER BY' in query_text:
            suggestions.append("ORDER BY ì ˆì˜ ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ ìƒì„± ê³ ë ¤")
        
        if 'GROUP BY' in query_text:
            suggestions.append("GROUP BY ì ˆì˜ ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ ìƒì„± ê³ ë ¤")
        
        # ì¿¼ë¦¬ êµ¬ì¡° ì œì•ˆ
        if 'SELECT *' in query_text:
            suggestions.append("í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ")
        
        if query_stat['mean_time'] > 5000:  # 5ì´ˆ ì´ìƒ
            suggestions.append("ì¿¼ë¦¬ê°€ ë§¤ìš° ëŠë¦¼ - ì¿¼ë¦¬ ì¬ì‘ì„± ë˜ëŠ” ë°ì´í„° íŒŒí‹°ì…”ë‹ ê³ ë ¤")
        
        if query_stat.get('temp_blks_written', 0) > 0:
            suggestions.append("ì„ì‹œ íŒŒì¼ ì‚¬ìš© ê°ì§€ - work_mem ì¦ê°€ ë˜ëŠ” ì¿¼ë¦¬ ìµœì í™” í•„ìš”")
        
        return suggestions

    async def recommend_indexes(self) -> List[IndexRecommendation]:
        """ì¸ë±ìŠ¤ ì¶”ì²œ"""
        logger.info("ğŸ“Š ì¸ë±ìŠ¤ ì¶”ì²œ ë¶„ì„ ì¤‘...")
        
        conn = self.get_connection()
        recommendations = []
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # í…Œì´ë¸”ë³„ ìŠ¤ìº” í†µê³„ ì¡°íšŒ
                cursor.execute("""
                    SELECT 
                        schemaname,
                        tablename,
                        seq_scan,
                        seq_tup_read,
                        idx_scan,
                        idx_tup_fetch,
                        n_tup_ins,
                        n_tup_upd,
                        n_tup_del
                    FROM pg_stat_user_tables
                    WHERE seq_scan > 1000 OR (seq_scan > idx_scan AND seq_scan > 10)
                    ORDER BY seq_scan DESC
                """)
                
                tables_needing_indexes = cursor.fetchall()
                
                for table_stat in tables_needing_indexes:
                    # í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ë¶„ì„
                    table_name = f"{table_stat['schemaname']}.{table_stat['tablename']}"
                    
                    # WHERE ì ˆì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ì»¬ëŸ¼ ì°¾ê¸°
                    frequent_columns = await self._find_frequent_where_columns(table_name)
                    
                    for column in frequent_columns:
                        recommendation = IndexRecommendation(
                            table_name=table_name,
                            column_names=[column],
                            index_type="btree",
                            expected_improvement=self._calculate_expected_improvement(table_stat),
                            usage_frequency=table_stat['seq_scan'],
                            size_estimate_mb=self._estimate_index_size(table_name, [column]),
                            creation_time_estimate=self._estimate_creation_time(table_name)
                        )
                        recommendations.append(recommendation)
                
                logger.info(f"âœ… {len(recommendations)}ê°œ ì¸ë±ìŠ¤ ì¶”ì²œ ìƒì„±")
                return recommendations
                
        except Exception as e:
            logger.error(f"âŒ ì¸ë±ìŠ¤ ì¶”ì²œ ì‹¤íŒ¨: {str(e)}")
            return []
        finally:
            self.return_connection(conn)

    async def _find_frequent_where_columns(self, table_name: str) -> List[str]:
        """WHERE ì ˆì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ì»¬ëŸ¼ ì°¾ê¸°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” pg_stat_statementsë¥¼ ë¶„ì„í•˜ì—¬
        # í•´ë‹¹ í…Œì´ë¸”ì— ëŒ€í•œ ì¿¼ë¦¬ë“¤ì˜ WHERE ì ˆì„ íŒŒì‹±í•´ì•¼ í•¨
        # ì—¬ê¸°ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ì»¬ëŸ¼ë“¤ì„ ë°˜í™˜
        
        common_columns = {
            'sensors.sensor_readings': ['created_at', 'device_id', 'vehicle_id'],
            'sensors.tpms_data': ['created_at', 'vehicle_id', 'tire_position'],
            'sensors.vehicles': ['status', 'created_at'],
            'alerts.alert_events': ['created_at', 'severity', 'vehicle_id'],
            'auth.users': ['email', 'username'],
            'auth.security_events': ['timestamp', 'source_ip']
        }
        
        return common_columns.get(table_name, ['id', 'created_at'])

    def _calculate_expected_improvement(self, table_stat: Dict) -> float:
        """ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ ê³„ì‚°"""
        seq_scan = table_stat['seq_scan']
        idx_scan = table_stat.get('idx_scan', 0)
        
        if seq_scan > 0:
            # ì‹œí€€ì…œ ìŠ¤ìº” ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ ì¸ë±ìŠ¤ë¡œ ì¸í•œ ê°œì„  íš¨ê³¼ê°€ í¼
            improvement = min(90.0, (seq_scan / (seq_scan + idx_scan + 1)) * 100)
            return improvement
        
        return 0.0

    def _estimate_index_size(self, table_name: str, columns: List[str]) -> float:
        """ì¸ë±ìŠ¤ í¬ê¸° ì˜ˆìƒ"""
        # ê°„ë‹¨í•œ ì¶”ì •ì‹ (ì‹¤ì œë¡œëŠ” í…Œì´ë¸” í¬ê¸°ì™€ ì»¬ëŸ¼ íƒ€ì…ì„ ê³ ë ¤í•´ì•¼ í•¨)
        base_size = 10.0  # MB
        column_factor = len(columns) * 2.0
        return base_size + column_factor

    def _estimate_creation_time(self, table_name: str) -> int:
        """ì¸ë±ìŠ¤ ìƒì„± ì‹œê°„ ì˜ˆìƒ (ì´ˆ)"""
        # í…Œì´ë¸” í¬ê¸°ì— ë”°ë¥¸ ëŒ€ëµì ì¸ ìƒì„± ì‹œê°„
        # ì‹¤ì œë¡œëŠ” í…Œì´ë¸” í¬ê¸°ë¥¼ ì¡°íšŒí•´ì•¼ í•¨
        return 30  # 30ì´ˆ ê¸°ë³¸ê°’

    async def create_recommended_indexes(self, recommendations: List[IndexRecommendation]) -> List[str]:
        """ì¶”ì²œëœ ì¸ë±ìŠ¤ ìƒì„±"""
        logger.info(f"ğŸ”¨ {len(recommendations)}ê°œ ì¶”ì²œ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        created_indexes = []
        conn = self.get_connection()
        
        try:
            with conn.cursor() as cursor:
                for rec in recommendations:
                    if rec.expected_improvement > 20.0:  # 20% ì´ìƒ ê°œì„  ì˜ˆìƒì‹œë§Œ ìƒì„±
                        index_name = f"idx_{rec.table_name.split('.')[-1]}_{('_'.join(rec.column_names))}"
                        columns_str = ', '.join(rec.column_names)
                        
                        create_sql = f"""
                            CREATE INDEX CONCURRENTLY IF NOT EXISTS {index_name}
                            ON {rec.table_name} ({columns_str})
                        """
                        
                        try:
                            start_time = time.time()
                            cursor.execute(create_sql)
                            execution_time = time.time() - start_time
                            
                            created_indexes.append(index_name)
                            
                            # ìµœì í™” ì´ë ¥ ê¸°ë¡
                            await self._record_optimization(
                                OptimizationType.INDEX_CREATION,
                                rec.table_name,
                                f"Created index {index_name}",
                                {},
                                {"index_name": index_name, "execution_time": execution_time},
                                rec.expected_improvement
                            )
                            
                            logger.info(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {index_name} ({execution_time:.2f}ì´ˆ)")
                            
                        except Exception as e:
                            logger.warning(f"âš ï¸ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {index_name} - {str(e)}")
            
            conn.commit()
            
        except Exception as e:
            logger.error(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            conn.rollback()
        finally:
            self.return_connection(conn)
        
        return created_indexes

    async def implement_partitioning(self, strategy: PartitionStrategy) -> bool:
        """íŒŒí‹°ì…”ë‹ êµ¬í˜„"""
        logger.info(f"ğŸ—‚ï¸ í…Œì´ë¸” íŒŒí‹°ì…”ë‹ êµ¬í˜„ ì¤‘: {strategy.table_name}")
        
        conn = self.get_connection()
        
        try:
            with conn.cursor() as cursor:
                # ê¸°ì¡´ í…Œì´ë¸” ë°±ì—…
                backup_table = f"{strategy.table_name}_backup_{int(time.time())}"
                
                cursor.execute(f"""
                    CREATE TABLE {backup_table} AS 
                    SELECT * FROM {strategy.table_name}
                """)
                
                # íŒŒí‹°ì…˜ í…Œì´ë¸” ìƒì„±
                if strategy.partition_type == "range":
                    await self._create_range_partitions(cursor, strategy)
                elif strategy.partition_type == "hash":
                    await self._create_hash_partitions(cursor, strategy)
                
                # ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
                await self._migrate_data_to_partitions(cursor, strategy, backup_table)
                
                # ìµœì í™” ì´ë ¥ ê¸°ë¡
                await self._record_optimization(
                    OptimizationType.PARTITIONING,
                    strategy.table_name,
                    f"Implemented {strategy.partition_type} partitioning on {strategy.partition_key}",
                    {"backup_table": backup_table},
                    {"partition_strategy": asdict(strategy)},
                    strategy.expected_performance_gain
                )
                
            conn.commit()
            logger.info(f"âœ… íŒŒí‹°ì…”ë‹ êµ¬í˜„ ì™„ë£Œ: {strategy.table_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ íŒŒí‹°ì…”ë‹ êµ¬í˜„ ì‹¤íŒ¨: {str(e)}")
            conn.rollback()
            return False
        finally:
            self.return_connection(conn)

    async def _create_range_partitions(self, cursor, strategy: PartitionStrategy):
        """ë²”ìœ„ íŒŒí‹°ì…˜ ìƒì„±"""
        # ì˜ˆ: ë‚ ì§œ ê¸°ë°˜ ì›”ë³„ íŒŒí‹°ì…˜
        if strategy.partition_key == 'created_at':
            # í˜„ì¬ ì›”ë¶€í„° 6ê°œì›” í›„ê¹Œì§€ íŒŒí‹°ì…˜ ìƒì„±
            for i in range(7):
                partition_date = datetime.utcnow() + timedelta(days=30*i)
                partition_name = f"{strategy.table_name}_{partition_date.strftime('%Y_%m')}"
                
                start_date = partition_date.replace(day=1)
                if i < 6:
                    end_date = (start_date + timedelta(days=32)).replace(day=1)
                else:
                    end_date = start_date + timedelta(days=365)  # ë§ˆì§€ë§‰ íŒŒí‹°ì…˜ì€ 1ë…„ í›„
                
                cursor.execute(f"""
                    CREATE TABLE {partition_name} PARTITION OF {strategy.table_name}
                    FOR VALUES FROM ('{start_date.isoformat()}') TO ('{end_date.isoformat()}')
                """)

    async def _create_hash_partitions(self, cursor, strategy: PartitionStrategy):
        """í•´ì‹œ íŒŒí‹°ì…˜ ìƒì„±"""
        # í•´ì‹œ íŒŒí‹°ì…˜ 4ê°œ ìƒì„±
        for i in range(4):
            partition_name = f"{strategy.table_name}_hash_{i}"
            cursor.execute(f"""
                CREATE TABLE {partition_name} PARTITION OF {strategy.table_name}
                FOR VALUES WITH (modulus 4, remainder {i})
            """)

    async def _migrate_data_to_partitions(self, cursor, strategy: PartitionStrategy, backup_table: str):
        """íŒŒí‹°ì…˜ìœ¼ë¡œ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
        # ë°±ì—… í…Œì´ë¸”ì—ì„œ ìƒˆ íŒŒí‹°ì…˜ í…Œì´ë¸”ë¡œ ë°ì´í„° ë³µì‚¬
        cursor.execute(f"""
            INSERT INTO {strategy.table_name} 
            SELECT * FROM {backup_table}
        """)

    async def setup_caching_strategy(self) -> Dict[str, Any]:
        """ìºì‹± ì „ëµ ì„¤ì •"""
        logger.info("ğŸ—„ï¸ ìºì‹± ì „ëµ ì„¤ì • ì¤‘...")
        
        # ìì£¼ ì¡°íšŒë˜ëŠ” ì¿¼ë¦¬ë“¤ì„ ìºì‹œì— ì €ì¥
        cache_strategies = {
            'dashboard_summary': {
                'ttl': 60,  # 1ë¶„
                'refresh_interval': 30
            },
            'vehicle_status': {
                'ttl': 30,  # 30ì´ˆ
                'refresh_interval': 15
            },
            'sensor_aggregates': {
                'ttl': 300,  # 5ë¶„
                'refresh_interval': 120
            },
            'alert_counts': {
                'ttl': 120,  # 2ë¶„
                'refresh_interval': 60
            }
        }
        
        # Redisì— ìºì‹± ì „ëµ ì €ì¥
        for key, strategy in cache_strategies.items():
            cache_key = f"cache_strategy:{key}"
            self.redis_client.hset(cache_key, mapping=strategy)
            self.redis_client.expire(cache_key, 86400)  # 24ì‹œê°„
        
        # ìºì‹œ ì›Œë°ì—…
        await self._warm_up_cache()
        
        logger.info("âœ… ìºì‹± ì „ëµ ì„¤ì • ì™„ë£Œ")
        return cache_strategies

    async def _warm_up_cache(self):
        """ìºì‹œ ì›Œë°ì—…"""
        logger.info("ğŸ”¥ ìºì‹œ ì›Œë°ì—… ì¤‘...")
        
        warmup_queries = [
            ("dashboard_summary", "SELECT COUNT(*) as total_vehicles, COUNT(CASE WHEN status = 'active' THEN 1 END) as active_vehicles FROM sensors.vehicles"),
            ("recent_alerts", "SELECT COUNT(*) FROM alerts.alert_events WHERE created_at > NOW() - INTERVAL '1 hour'"),
            ("sensor_counts", "SELECT COUNT(*) FROM sensors.sensor_readings WHERE created_at > NOW() - INTERVAL '1 hour'")
        ]
        
        conn = self.get_connection()
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                for cache_key, query in warmup_queries:
                    try:
                        cursor.execute(query)
                        result = cursor.fetchall()
                        
                        # Redisì— ê²°ê³¼ ìºì‹œ
                        self.redis_client.setex(
                            f"cache:{cache_key}",
                            CACHE_TTL_SECONDS,
                            json.dumps(result, default=str)
                        )
                        
                        logger.debug(f"âœ… ìºì‹œ ì›Œë°ì—…: {cache_key}")
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ ìºì‹œ ì›Œë°ì—… ì‹¤íŒ¨: {cache_key} - {str(e)}")
                        
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ì›Œë°ì—… ì‹¤íŒ¨: {str(e)}")
        finally:
            self.return_connection(conn)

    async def run_maintenance_tasks(self):
        """ì •ê¸° ìœ ì§€ë³´ìˆ˜ ì‘ì—…"""
        logger.info("ğŸ§¹ ë°ì´í„°ë² ì´ìŠ¤ ìœ ì§€ë³´ìˆ˜ ì‘ì—… ì‹¤í–‰ ì¤‘...")
        
        conn = self.get_connection()
        
        try:
            with conn.cursor() as cursor:
                # VACUUM ANALYZE ì‹¤í–‰
                tables_to_maintain = [
                    'sensors.sensor_readings',
                    'sensors.tpms_data',
                    'alerts.alert_events',
                    'auth.security_events'
                ]
                
                for table in tables_to_maintain:
                    try:
                        start_time = time.time()
                        cursor.execute(f"VACUUM ANALYZE {table}")
                        execution_time = time.time() - start_time
                        
                        logger.info(f"âœ… VACUUM ANALYZE ì™„ë£Œ: {table} ({execution_time:.2f}ì´ˆ)")
                        
                        # ìµœì í™” ì´ë ¥ ê¸°ë¡
                        await self._record_optimization(
                            OptimizationType.VACUUM_ANALYZE,
                            table,
                            "VACUUM ANALYZE executed",
                            {},
                            {"execution_time": execution_time},
                            5.0  # 5% ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ
                        )
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ VACUUM ANALYZE ì‹¤íŒ¨: {table} - {str(e)}")
                
                # í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸
                cursor.execute("ANALYZE")
                
            conn.commit()
            
        except Exception as e:
            logger.error(f"âŒ ìœ ì§€ë³´ìˆ˜ ì‘ì—… ì‹¤íŒ¨: {str(e)}")
            conn.rollback()
        finally:
            self.return_connection(conn)

    async def _record_optimization(self, opt_type: OptimizationType, target: str, action: str,
                                 before_metrics: Dict, after_metrics: Dict, performance_gain: float):
        """ìµœì í™” ì´ë ¥ ê¸°ë¡"""
        conn = self.get_connection()
        
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO performance.optimization_history 
                    (optimization_type, target_object, action_taken, before_metrics, after_metrics, performance_gain)
                    VALUES (%s, %s, %s, %s, %s, %s)
                """, (
                    opt_type.value,
                    target,
                    action,
                    json.dumps(before_metrics),
                    json.dumps(after_metrics),
                    performance_gain
                ))
                
            conn.commit()
            
        except Exception as e:
            logger.error(f"âŒ ìµœì í™” ì´ë ¥ ê¸°ë¡ ì‹¤íŒ¨: {str(e)}")
            conn.rollback()
        finally:
            self.return_connection(conn)

    async def generate_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        logger.info("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        conn = self.get_connection()
        
        try:
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                report = {
                    'generated_at': datetime.utcnow().isoformat(),
                    'database_size': {},
                    'query_performance': {},
                    'index_usage': {},
                    'optimization_history': [],
                    'recommendations': []
                }
                
                # ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° ì •ë³´
                cursor.execute("""
                    SELECT 
                        pg_size_pretty(pg_database_size(current_database())) as database_size,
                        pg_size_pretty(sum(pg_total_relation_size(c.oid))) as tables_size
                    FROM pg_class c
                    LEFT JOIN pg_namespace n ON n.oid = c.relnamespace
                    WHERE n.nspname NOT IN ('information_schema', 'pg_catalog', 'pg_toast')
                      AND c.relkind = 'r'
                """)
                
                size_info = cursor.fetchone()
                report['database_size'] = dict(size_info) if size_info else {}
                
                # ìƒìœ„ ëŠë¦° ì¿¼ë¦¬
                cursor.execute("""
                    SELECT 
                        query,
                        calls,
                        total_time,
                        mean_time,
                        max_time
                    FROM pg_stat_statements 
                    ORDER BY total_time DESC 
                    LIMIT 10
                """)
                
                slow_queries = cursor.fetchall()
                report['query_performance']['slow_queries'] = [dict(q) for q in slow_queries]
                
                # ì¸ë±ìŠ¤ ì‚¬ìš©ë¥ 
                cursor.execute("""
                    SELECT 
                        schemaname,
                        tablename,
                        indexname,
                        idx_scan,
                        idx_tup_read,
                        idx_tup_fetch
                    FROM pg_stat_user_indexes
                    ORDER BY idx_scan DESC
                    LIMIT 20
                """)
                
                index_stats = cursor.fetchall()
                report['index_usage'] = [dict(idx) for idx in index_stats]
                
                # ìµœê·¼ ìµœì í™” ì´ë ¥
                cursor.execute("""
                    SELECT *
                    FROM performance.optimization_history
                    ORDER BY timestamp DESC
                    LIMIT 20
                """)
                
                optimization_history = cursor.fetchall()
                report['optimization_history'] = [dict(opt) for opt in optimization_history]
                
                return report
                
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {}
        finally:
            self.return_connection(conn)

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ HankookTire SmartSensor 2.0 ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹œì‘")
    
    optimizer = DatabaseOptimizer()
    await optimizer.initialize()
    
    # ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„
    query_analyses = await optimizer.analyze_query_performance()
    logger.info(f"ğŸ“Š ë¶„ì„ëœ ì¿¼ë¦¬ ìˆ˜: {len(query_analyses)}")
    
    # ì¸ë±ìŠ¤ ì¶”ì²œ ë° ìƒì„±
    recommendations = await optimizer.recommend_indexes()
    if recommendations:
        created_indexes = await optimizer.create_recommended_indexes(recommendations)
        logger.info(f"ğŸ”¨ ìƒì„±ëœ ì¸ë±ìŠ¤ ìˆ˜: {len(created_indexes)}")
    
    # ìºì‹± ì „ëµ ì„¤ì •
    cache_strategies = await optimizer.setup_caching_strategy()
    logger.info(f"ğŸ—„ï¸ ì„¤ì •ëœ ìºì‹œ ì „ëµ ìˆ˜: {len(cache_strategies)}")
    
    # ìœ ì§€ë³´ìˆ˜ ì‘ì—…
    await optimizer.run_maintenance_tasks()
    
    # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
    report = await optimizer.generate_performance_report()
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    with open('database_performance_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)
    
    logger.info("ğŸ‰ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì™„ë£Œ!")
    logger.info("ğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸: database_performance_report.json")

if __name__ == "__main__":
    asyncio.run(main())