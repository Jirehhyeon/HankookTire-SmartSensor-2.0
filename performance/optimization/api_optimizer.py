#!/usr/bin/env python3
"""
HankookTire SmartSensor 2.0 - API Performance Optimizer
ì°¨ì„¸ëŒ€ í†µí•© ìŠ¤ë§ˆíŠ¸ íƒ€ì´ì–´ ì„¼ì„œ ì‹œìŠ¤í…œ API ì„±ëŠ¥ ìµœì í™”

API ì„±ëŠ¥ ìµœì í™” ë„êµ¬
- ì‘ë‹µ ì‹œê°„ ìµœì í™”
- ë™ì‹œì„± ì²˜ë¦¬ ê°œì„ 
- ìºì‹± ì „ëµ êµ¬í˜„
- ì••ì¶• ë° ì§ë ¬í™” ìµœì í™”
- ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”
- API ì†ë„ ì œí•œ ìµœì í™”
"""

import asyncio
import aiohttp
from aiohttp import web, web_middlewares
import aioredis
import json
import logging
import time
import gzip
import pickle
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Callable
from dataclasses import dataclass, asdict
from enum import Enum
import statistics
import threading
from concurrent.futures import ThreadPoolExecutor
import uvloop
import orjson
import msgpack
from functools import wraps, lru_cache
import hashlib
import psutil
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CacheStrategy(Enum):
    """ìºì‹œ ì „ëµ"""
    NO_CACHE = "no_cache"
    MEMORY_CACHE = "memory_cache"
    REDIS_CACHE = "redis_cache"
    HYBRID_CACHE = "hybrid_cache"
    WRITE_THROUGH = "write_through"
    WRITE_BEHIND = "write_behind"

class CompressionType(Enum):
    """ì••ì¶• íƒ€ì…"""
    NONE = "none"
    GZIP = "gzip"
    BROTLI = "brotli"
    LZ4 = "lz4"

class SerializationType(Enum):
    """ì§ë ¬í™” íƒ€ì…"""
    JSON = "json"
    ORJSON = "orjson"
    MSGPACK = "msgpack"
    PICKLE = "pickle"

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    endpoint: str
    method: str
    response_time_ms: float
    memory_usage_mb: float
    cpu_usage_percent: float
    cache_hit: bool
    compression_ratio: float
    request_size_bytes: int
    response_size_bytes: int
    concurrent_requests: int
    timestamp: datetime

@dataclass
class OptimizationConfig:
    """ìµœì í™” ì„¤ì •"""
    cache_strategy: CacheStrategy
    cache_ttl_seconds: int
    compression_type: CompressionType
    compression_threshold: int  # bytes
    serialization_type: SerializationType
    max_concurrent_requests: int
    request_timeout_seconds: int
    enable_async_processing: bool
    enable_response_streaming: bool

class CacheManager:
    """ìºì‹œ ê´€ë¦¬ì"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_url = redis_url
        self.redis = None
        self.memory_cache = {}
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'deletes': 0
        }
        
    async def initialize(self):
        """ì´ˆê¸°í™”"""
        try:
            self.redis = aioredis.from_url(self.redis_url)
            await self.redis.ping()
            logger.info("âœ… Redis ì—°ê²° ì„±ê³µ")
        except Exception as e:
            logger.warning(f"âš ï¸ Redis ì—°ê²° ì‹¤íŒ¨: {str(e)} - ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©")
            self.redis = None
    
    async def get(self, key: str, strategy: CacheStrategy = CacheStrategy.HYBRID_CACHE) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        try:
            if strategy == CacheStrategy.NO_CACHE:
                return None
            
            # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
            if strategy in [CacheStrategy.MEMORY_CACHE, CacheStrategy.HYBRID_CACHE]:
                if key in self.memory_cache:
                    self.cache_stats['hits'] += 1
                    return self.memory_cache[key]['data']
            
            # Redis ìºì‹œ í™•ì¸
            if self.redis and strategy in [CacheStrategy.REDIS_CACHE, CacheStrategy.HYBRID_CACHE]:
                data = await self.redis.get(key)
                if data:
                    self.cache_stats['hits'] += 1
                    return orjson.loads(data)
            
            self.cache_stats['misses'] += 1
            return None
            
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            self.cache_stats['misses'] += 1
            return None
    
    async def set(self, key: str, value: Any, ttl: int = 300, 
                 strategy: CacheStrategy = CacheStrategy.HYBRID_CACHE):
        """ìºì‹œì— ê°’ ì €ì¥"""
        try:
            if strategy == CacheStrategy.NO_CACHE:
                return
            
            # ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥
            if strategy in [CacheStrategy.MEMORY_CACHE, CacheStrategy.HYBRID_CACHE]:
                self.memory_cache[key] = {
                    'data': value,
                    'expires_at': time.time() + ttl
                }
                
                # ë©”ëª¨ë¦¬ ìºì‹œ í¬ê¸° ì œí•œ (1000ê°œ)
                if len(self.memory_cache) > 1000:
                    oldest_key = min(self.memory_cache.keys(), 
                                   key=lambda k: self.memory_cache[k]['expires_at'])
                    del self.memory_cache[oldest_key]
            
            # Redis ìºì‹œ ì €ì¥
            if self.redis and strategy in [CacheStrategy.REDIS_CACHE, CacheStrategy.HYBRID_CACHE]:
                await self.redis.setex(key, ttl, orjson.dumps(value))
            
            self.cache_stats['sets'] += 1
            
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    async def delete(self, key: str, strategy: CacheStrategy = CacheStrategy.HYBRID_CACHE):
        """ìºì‹œì—ì„œ ê°’ ì‚­ì œ"""
        try:
            # ë©”ëª¨ë¦¬ ìºì‹œ ì‚­ì œ
            if strategy in [CacheStrategy.MEMORY_CACHE, CacheStrategy.HYBRID_CACHE]:
                if key in self.memory_cache:
                    del self.memory_cache[key]
            
            # Redis ìºì‹œ ì‚­ì œ
            if self.redis and strategy in [CacheStrategy.REDIS_CACHE, CacheStrategy.HYBRID_CACHE]:
                await self.redis.delete(key)
            
            self.cache_stats['deletes'] += 1
            
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = self.cache_stats['hits'] / total_requests if total_requests > 0 else 0
        
        return {
            **self.cache_stats,
            'hit_rate': hit_rate,
            'memory_cache_size': len(self.memory_cache)
        }

class CompressionManager:
    """ì••ì¶• ê´€ë¦¬ì"""
    
    @staticmethod
    def compress(data: bytes, compression_type: CompressionType) -> Tuple[bytes, float]:
        """ë°ì´í„° ì••ì¶•"""
        original_size = len(data)
        
        if compression_type == CompressionType.NONE:
            return data, 1.0
        
        try:
            if compression_type == CompressionType.GZIP:
                compressed = gzip.compress(data)
            else:
                # ë‹¤ë¥¸ ì••ì¶• ì•Œê³ ë¦¬ì¦˜ë“¤ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì ì ˆí•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
                compressed = gzip.compress(data)  # ê¸°ë³¸ê°’ìœ¼ë¡œ gzip ì‚¬ìš©
            
            compression_ratio = len(compressed) / original_size if original_size > 0 else 1.0
            return compressed, compression_ratio
            
        except Exception as e:
            logger.error(f"âŒ ì••ì¶• ì‹¤íŒ¨: {str(e)}")
            return data, 1.0
    
    @staticmethod
    def decompress(data: bytes, compression_type: CompressionType) -> bytes:
        """ë°ì´í„° ì••ì¶• í•´ì œ"""
        if compression_type == CompressionType.NONE:
            return data
        
        try:
            if compression_type == CompressionType.GZIP:
                return gzip.decompress(data)
            else:
                return gzip.decompress(data)  # ê¸°ë³¸ê°’ìœ¼ë¡œ gzip ì‚¬ìš©
                
        except Exception as e:
            logger.error(f"âŒ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {str(e)}")
            return data

class SerializationManager:
    """ì§ë ¬í™” ê´€ë¦¬ì"""
    
    @staticmethod
    def serialize(data: Any, serialization_type: SerializationType) -> bytes:
        """ë°ì´í„° ì§ë ¬í™”"""
        try:
            if serialization_type == SerializationType.JSON:
                return json.dumps(data, default=str).encode('utf-8')
            elif serialization_type == SerializationType.ORJSON:
                return orjson.dumps(data)
            elif serialization_type == SerializationType.MSGPACK:
                return msgpack.packb(data)
            elif serialization_type == SerializationType.PICKLE:
                return pickle.dumps(data)
            else:
                return orjson.dumps(data)  # ê¸°ë³¸ê°’
                
        except Exception as e:
            logger.error(f"âŒ ì§ë ¬í™” ì‹¤íŒ¨: {str(e)}")
            return b'{}'
    
    @staticmethod
    def deserialize(data: bytes, serialization_type: SerializationType) -> Any:
        """ë°ì´í„° ì—­ì§ë ¬í™”"""
        try:
            if serialization_type == SerializationType.JSON:
                return json.loads(data.decode('utf-8'))
            elif serialization_type == SerializationType.ORJSON:
                return orjson.loads(data)
            elif serialization_type == SerializationType.MSGPACK:
                return msgpack.unpackb(data)
            elif serialization_type == SerializationType.PICKLE:
                return pickle.loads(data)
            else:
                return orjson.loads(data)  # ê¸°ë³¸ê°’
                
        except Exception as e:
            logger.error(f"âŒ ì—­ì§ë ¬í™” ì‹¤íŒ¨: {str(e)}")
            return {}

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°"""
    
    def __init__(self):
        self.metrics = []
        self.active_requests = 0
        self.request_lock = threading.Lock()
        
    def start_request(self) -> str:
        """ìš”ì²­ ì‹œì‘ ê¸°ë¡"""
        with self.request_lock:
            self.active_requests += 1
            request_id = f"req_{int(time.time() * 1000)}_{self.active_requests}"
            return request_id
    
    def end_request(self):
        """ìš”ì²­ ì¢…ë£Œ ê¸°ë¡"""
        with self.request_lock:
            self.active_requests = max(0, self.active_requests - 1)
    
    def record_metric(self, metric: PerformanceMetrics):
        """ë©”íŠ¸ë¦­ ê¸°ë¡"""
        self.metrics.append(metric)
        
        # ë©”íŠ¸ë¦­ ê°œìˆ˜ ì œí•œ (ìµœê·¼ 10000ê°œë§Œ ìœ ì§€)
        if len(self.metrics) > 10000:
            self.metrics = self.metrics[-5000:]
    
    def get_stats(self, last_minutes: int = 5) -> Dict[str, Any]:
        """í†µê³„ ì¡°íšŒ"""
        cutoff_time = datetime.utcnow() - timedelta(minutes=last_minutes)
        recent_metrics = [m for m in self.metrics if m.timestamp > cutoff_time]
        
        if not recent_metrics:
            return {}
        
        response_times = [m.response_time_ms for m in recent_metrics]
        memory_usage = [m.memory_usage_mb for m in recent_metrics]
        cache_hits = sum(1 for m in recent_metrics if m.cache_hit)
        
        return {
            'total_requests': len(recent_metrics),
            'active_requests': self.active_requests,
            'avg_response_time_ms': statistics.mean(response_times),
            'p95_response_time_ms': statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else max(response_times),
            'p99_response_time_ms': statistics.quantiles(response_times, n=100)[98] if len(response_times) > 100 else max(response_times),
            'avg_memory_usage_mb': statistics.mean(memory_usage),
            'cache_hit_rate': cache_hits / len(recent_metrics),
            'requests_per_minute': len(recent_metrics) / last_minutes
        }

class APIOptimizer:
    """API ìµœì í™” ê´€ë¦¬ì"""
    
    def __init__(self):
        self.cache_manager = CacheManager()
        self.performance_monitor = PerformanceMonitor()
        self.optimization_configs = {}
        self.executor = ThreadPoolExecutor(max_workers=10)
        
    async def initialize(self):
        """ì´ˆê¸°í™”"""
        await self.cache_manager.initialize()
        
        # ê¸°ë³¸ ìµœì í™” ì„¤ì •
        default_config = OptimizationConfig(
            cache_strategy=CacheStrategy.HYBRID_CACHE,
            cache_ttl_seconds=300,
            compression_type=CompressionType.GZIP,
            compression_threshold=1024,  # 1KB
            serialization_type=SerializationType.ORJSON,
            max_concurrent_requests=100,
            request_timeout_seconds=30,
            enable_async_processing=True,
            enable_response_streaming=False
        )
        
        self.optimization_configs['default'] = default_config
        
        logger.info("âœ… API ìµœì í™” ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")

    def optimized_endpoint(self, cache_key: Optional[str] = None, 
                          config_name: str = 'default'):
        """ìµœì í™”ëœ ì—”ë“œí¬ì¸íŠ¸ ë°ì½”ë ˆì´í„°"""
        def decorator(func: Callable):
            @wraps(func)
            async def wrapper(request: web.Request):
                start_time = time.time()
                request_id = self.performance_monitor.start_request()
                
                config = self.optimization_configs.get(config_name, self.optimization_configs['default'])
                
                try:
                    # ìºì‹œ í™•ì¸
                    if cache_key:
                        cache_data = await self._get_cached_response(request, cache_key, config)
                        if cache_data:
                            return await self._create_optimized_response(
                                cache_data, config, start_time, request_id, 
                                request.path, request.method, True
                            )
                    
                    # ì‹¤ì œ ì²˜ë¦¬ ì‹¤í–‰
                    if config.enable_async_processing:
                        response_data = await func(request)
                    else:
                        response_data = await asyncio.get_event_loop().run_in_executor(
                            self.executor, lambda: asyncio.run(func(request))
                        )
                    
                    # ìºì‹œ ì €ì¥
                    if cache_key:
                        await self._cache_response(request, cache_key, response_data, config)
                    
                    return await self._create_optimized_response(
                        response_data, config, start_time, request_id,
                        request.path, request.method, False
                    )
                    
                except Exception as e:
                    logger.error(f"âŒ API ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                    return web.json_response(
                        {'error': 'Internal server error'}, 
                        status=500
                    )
                finally:
                    self.performance_monitor.end_request()
                    
            return wrapper
        return decorator

    async def _get_cached_response(self, request: web.Request, cache_key: str, 
                                 config: OptimizationConfig) -> Optional[Any]:
        """ìºì‹œëœ ì‘ë‹µ ì¡°íšŒ"""
        full_cache_key = self._build_cache_key(request, cache_key)
        return await self.cache_manager.get(full_cache_key, config.cache_strategy)

    async def _cache_response(self, request: web.Request, cache_key: str, 
                            response_data: Any, config: OptimizationConfig):
        """ì‘ë‹µ ìºì‹œ ì €ì¥"""
        full_cache_key = self._build_cache_key(request, cache_key)
        await self.cache_manager.set(
            full_cache_key, response_data, 
            config.cache_ttl_seconds, config.cache_strategy
        )

    def _build_cache_key(self, request: web.Request, base_key: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ìš”ì²­ íŒŒë¼ë¯¸í„°ë¥¼ í¬í•¨í•œ ìºì‹œ í‚¤ ìƒì„±
        params = dict(request.query)
        params_str = json.dumps(params, sort_keys=True)
        cache_key = f"{base_key}:{request.path}:{hashlib.md5(params_str.encode()).hexdigest()}"
        return cache_key

    async def _create_optimized_response(self, data: Any, config: OptimizationConfig,
                                       start_time: float, request_id: str,
                                       path: str, method: str, cache_hit: bool) -> web.Response:
        """ìµœì í™”ëœ ì‘ë‹µ ìƒì„±"""
        # ì§ë ¬í™”
        serialized_data = SerializationManager.serialize(data, config.serialization_type)
        
        # ì••ì¶• (ì„ê³„ê°’ ì´ìƒì¼ ë•Œë§Œ)
        compressed_data = serialized_data
        compression_ratio = 1.0
        content_encoding = None
        
        if len(serialized_data) > config.compression_threshold:
            compressed_data, compression_ratio = CompressionManager.compress(
                serialized_data, config.compression_type
            )
            if config.compression_type == CompressionType.GZIP:
                content_encoding = 'gzip'
        
        # ì‘ë‹µ í—¤ë” ì„¤ì •
        headers = {
            'Content-Type': 'application/json',
            'X-Request-ID': request_id,
            'X-Cache-Hit': str(cache_hit),
            'X-Compression-Ratio': f"{compression_ratio:.3f}"
        }
        
        if content_encoding:
            headers['Content-Encoding'] = content_encoding
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
        end_time = time.time()
        response_time_ms = (end_time - start_time) * 1000
        
        process = psutil.Process()
        memory_usage_mb = process.memory_info().rss / 1024 / 1024
        cpu_usage_percent = process.cpu_percent()
        
        metric = PerformanceMetrics(
            endpoint=path,
            method=method,
            response_time_ms=response_time_ms,
            memory_usage_mb=memory_usage_mb,
            cpu_usage_percent=cpu_usage_percent,
            cache_hit=cache_hit,
            compression_ratio=compression_ratio,
            request_size_bytes=0,  # ìš”ì²­ í¬ê¸°ëŠ” ë³„ë„ ì¸¡ì • í•„ìš”
            response_size_bytes=len(compressed_data),
            concurrent_requests=self.performance_monitor.active_requests,
            timestamp=datetime.utcnow()
        )
        
        self.performance_monitor.record_metric(metric)
        
        return web.Response(
            body=compressed_data,
            headers=headers,
            status=200
        )

    def add_optimization_config(self, name: str, config: OptimizationConfig):
        """ìµœì í™” ì„¤ì • ì¶”ê°€"""
        self.optimization_configs[name] = config
        logger.info(f"âœ… ìµœì í™” ì„¤ì • ì¶”ê°€: {name}")

    async def optimize_database_queries(self, query_func: Callable, 
                                      cache_key: str, ttl: int = 300):
        """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”"""
        # ìºì‹œ í™•ì¸
        cached_result = await self.cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        start_time = time.time()
        result = await query_func()
        execution_time = time.time() - start_time
        
        # ê²°ê³¼ ìºì‹œ ì €ì¥ (ì‹¤í–‰ ì‹œê°„ì´ ê¸´ ì¿¼ë¦¬ë§Œ)
        if execution_time > 0.1:  # 100ms ì´ìƒ
            await self.cache_manager.set(cache_key, result, ttl)
        
        return result

    async def batch_process_requests(self, requests: List[Dict], 
                                   process_func: Callable) -> List[Any]:
        """ìš”ì²­ ë°°ì¹˜ ì²˜ë¦¬"""
        batch_size = 50  # ë°°ì¹˜ í¬ê¸°
        results = []
        
        for i in range(0, len(requests), batch_size):
            batch = requests[i:i + batch_size]
            
            # ë°°ì¹˜ ë‚´ ìš”ì²­ë“¤ì„ ë³‘ë ¬ ì²˜ë¦¬
            tasks = [process_func(req) for req in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            results.extend(batch_results)
        
        return results

    async def precompute_expensive_operations(self):
        """ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì—°ì‚° ì‚¬ì „ ê³„ì‚°"""
        logger.info("ğŸ”„ ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì—°ì‚° ì‚¬ì „ ê³„ì‚° ì¤‘...")
        
        precompute_tasks = [
            ('dashboard_summary', self._precompute_dashboard_summary),
            ('vehicle_statistics', self._precompute_vehicle_statistics),
            ('sensor_aggregates', self._precompute_sensor_aggregates),
            ('alert_summaries', self._precompute_alert_summaries)
        ]
        
        for cache_key, compute_func in precompute_tasks:
            try:
                result = await compute_func()
                await self.cache_manager.set(cache_key, result, 3600)  # 1ì‹œê°„ ìºì‹œ
                logger.info(f"âœ… ì‚¬ì „ ê³„ì‚° ì™„ë£Œ: {cache_key}")
            except Exception as e:
                logger.error(f"âŒ ì‚¬ì „ ê³„ì‚° ì‹¤íŒ¨: {cache_key} - {str(e)}")

    async def _precompute_dashboard_summary(self) -> Dict[str, Any]:
        """ëŒ€ì‹œë³´ë“œ ìš”ì•½ ì‚¬ì „ ê³„ì‚°"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì‹¤í–‰
        return {
            'total_vehicles': 10000,
            'active_vehicles': 8500,
            'total_sensors': 40000,
            'active_sensors': 38000,
            'recent_alerts': 45,
            'critical_alerts': 3,
            'last_updated': datetime.utcnow().isoformat()
        }

    async def _precompute_vehicle_statistics(self) -> Dict[str, Any]:
        """ì°¨ëŸ‰ í†µê³„ ì‚¬ì „ ê³„ì‚°"""
        return {
            'by_status': {
                'active': 8500,
                'inactive': 1200,
                'maintenance': 300
            },
            'by_region': {
                'seoul': 3000,
                'busan': 1500,
                'incheon': 1200,
                'daegu': 1000,
                'others': 3300
            },
            'last_updated': datetime.utcnow().isoformat()
        }

    async def _precompute_sensor_aggregates(self) -> Dict[str, Any]:
        """ì„¼ì„œ ì§‘ê³„ ì‚¬ì „ ê³„ì‚°"""
        return {
            'average_pressure': 245.5,
            'average_temperature': 34.2,
            'data_points_today': 1250000,
            'quality_score': 97.8,
            'last_updated': datetime.utcnow().isoformat()
        }

    async def _precompute_alert_summaries(self) -> Dict[str, Any]:
        """ì•Œë¦¼ ìš”ì•½ ì‚¬ì „ ê³„ì‚°"""
        return {
            'today': {
                'total': 245,
                'critical': 8,
                'warning': 67,
                'info': 170
            },
            'this_week': {
                'total': 1680,
                'critical': 45,
                'warning': 456,
                'info': 1179
            },
            'last_updated': datetime.utcnow().isoformat()
        }

    async def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        performance_stats = self.performance_monitor.get_stats()
        cache_stats = self.cache_manager.get_stats()
        
        return {
            'generated_at': datetime.utcnow().isoformat(),
            'performance': performance_stats,
            'cache': cache_stats,
            'optimization_configs': {
                name: asdict(config) 
                for name, config in self.optimization_configs.items()
            }
        }

class RateLimiter:
    """API ì†ë„ ì œí•œê¸°"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
        
    async def is_allowed(self, key: str, limit: int, window: int) -> Tuple[bool, Dict[str, int]]:
        """ì†ë„ ì œí•œ í™•ì¸"""
        try:
            current_time = int(time.time())
            window_start = current_time - window
            
            # Redisì—ì„œ í˜„ì¬ ìœˆë„ìš°ì˜ ìš”ì²­ ìˆ˜ í™•ì¸
            pipe = self.redis.pipeline()
            pipe.zremrangebyscore(key, 0, window_start)
            pipe.zcard(key)
            pipe.zadd(key, {str(current_time): current_time})
            pipe.expire(key, window)
            
            results = await pipe.execute()
            current_requests = results[1]
            
            allowed = current_requests < limit
            remaining = max(0, limit - current_requests - 1)
            
            return allowed, {
                'limit': limit,
                'remaining': remaining,
                'reset_time': current_time + window
            }
            
        except Exception as e:
            logger.error(f"âŒ ì†ë„ ì œí•œ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
            return True, {'limit': limit, 'remaining': limit - 1, 'reset_time': int(time.time()) + window}

# ë¯¸ë“¤ì›¨ì–´
async def performance_middleware(request: web.Request, handler):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¯¸ë“¤ì›¨ì–´"""
    start_time = time.time()
    
    try:
        response = await handler(request)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ í—¤ë” ì¶”ê°€
        response.headers['X-Response-Time'] = f"{(time.time() - start_time) * 1000:.2f}ms"
        response.headers['X-Timestamp'] = str(int(time.time()))
        
        return response
        
    except Exception as e:
        logger.error(f"âŒ ìš”ì²­ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        raise

async def compression_middleware(request: web.Request, handler):
    """ì••ì¶• ë¯¸ë“¤ì›¨ì–´"""
    response = await handler(request)
    
    # Accept-Encoding í—¤ë” í™•ì¸
    accept_encoding = request.headers.get('Accept-Encoding', '')
    
    if 'gzip' in accept_encoding and len(response.body) > 1024:
        compressed_body = gzip.compress(response.body)
        response.body = compressed_body
        response.headers['Content-Encoding'] = 'gzip'
        response.headers['Content-Length'] = str(len(compressed_body))
    
    return response

async def cache_middleware(request: web.Request, handler):
    """ìºì‹œ ë¯¸ë“¤ì›¨ì–´"""
    # GET ìš”ì²­ì— ëŒ€í•´ì„œë§Œ ìºì‹œ ì ìš©
    if request.method != 'GET':
        return await handler(request)
    
    # ìºì‹œ í—¤ë” ì„¤ì •
    response = await handler(request)
    
    if response.status == 200:
        response.headers['Cache-Control'] = 'public, max-age=300'
        response.headers['ETag'] = hashlib.md5(response.body).hexdigest()
    
    return response

# ìµœì í™”ëœ API ì˜ˆì œ
async def create_optimized_app(optimizer: APIOptimizer):
    """ìµœì í™”ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±"""
    app = web.Application(middlewares=[
        performance_middleware,
        compression_middleware,
        cache_middleware
    ])
    
    # ìµœì í™”ëœ ì—”ë“œí¬ì¸íŠ¸ë“¤
    @optimizer.optimized_endpoint(cache_key='dashboard_summary')
    async def dashboard_summary(request: web.Request):
        """ëŒ€ì‹œë³´ë“œ ìš”ì•½ (ìºì‹œë¨)"""
        return await optimizer._precompute_dashboard_summary()
    
    @optimizer.optimized_endpoint(cache_key='vehicle_list')
    async def vehicle_list(request: web.Request):
        """ì°¨ëŸ‰ ëª©ë¡ (ìºì‹œë¨)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬
        vehicles = [
            {'id': i, 'status': 'active', 'location': f'Location {i}'}
            for i in range(1, 101)
        ]
        return {'vehicles': vehicles, 'total': len(vehicles)}
    
    @optimizer.optimized_endpoint()
    async def sensor_data(request: web.Request):
        """ì„¼ì„œ ë°ì´í„° (ìºì‹œ ì—†ìŒ)"""
        vehicle_id = request.query.get('vehicle_id')
        if not vehicle_id:
            return {'error': 'vehicle_id required'}
        
        # ì‹¤ì‹œê°„ ì„¼ì„œ ë°ì´í„° (ìºì‹œí•˜ì§€ ì•ŠìŒ)
        return {
            'vehicle_id': vehicle_id,
            'sensors': [
                {'type': 'pressure', 'value': 245.5, 'unit': 'kPa'},
                {'type': 'temperature', 'value': 34.2, 'unit': 'Â°C'}
            ],
            'timestamp': datetime.utcnow().isoformat()
        }
    
    async def performance_stats(request: web.Request):
        """ì„±ëŠ¥ í†µê³„"""
        report = await optimizer.get_performance_report()
        return web.json_response(report)
    
    # ë¼ìš°íŠ¸ ë“±ë¡
    app.router.add_get('/api/dashboard/summary', dashboard_summary)
    app.router.add_get('/api/vehicles', vehicle_list)
    app.router.add_get('/api/sensors/data', sensor_data)
    app.router.add_get('/api/performance/stats', performance_stats)
    
    return app

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ HankookTire SmartSensor 2.0 API ìµœì í™” ì‹œì‘")
    
    # uvloop ì‚¬ìš© (ì„±ëŠ¥ í–¥ìƒ)
    if hasattr(uvloop, 'install'):
        uvloop.install()
    
    # API ìµœì í™” ê´€ë¦¬ì ì´ˆê¸°í™”
    optimizer = APIOptimizer()
    await optimizer.initialize()
    
    # ì‚¬ì „ ê³„ì‚° ì‹¤í–‰
    await optimizer.precompute_expensive_operations()
    
    # ìµœì í™”ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
    app = await create_optimized_app(optimizer)
    
    # ì„œë²„ ì‹œì‘
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', 8000)
    
    logger.info("ğŸŒ ìµœì í™”ëœ API ì„œë²„ ì‹œì‘ - Port 8000")
    await site.start()
    
    try:
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë£¨í”„
        while True:
            await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤
            
            # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
            report = await optimizer.get_performance_report()
            
            logger.info(f"ğŸ“Š ì„±ëŠ¥ í†µê³„:")
            if 'performance' in report:
                perf = report['performance']
                logger.info(f"   í™œì„± ìš”ì²­: {perf.get('active_requests', 0)}")
                logger.info(f"   ë¶„ë‹¹ ìš”ì²­: {perf.get('requests_per_minute', 0):.1f}")
                logger.info(f"   í‰ê·  ì‘ë‹µì‹œê°„: {perf.get('avg_response_time_ms', 0):.2f}ms")
                logger.info(f"   ìºì‹œ ì ì¤‘ë¥ : {perf.get('cache_hit_rate', 0):.2%}")
            
            # ì‚¬ì „ ê³„ì‚° ê°±ì‹ 
            await optimizer.precompute_expensive_operations()
            
    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ API ì„œë²„ ì¢…ë£Œ ì¤‘...")
    finally:
        await runner.cleanup()

if __name__ == "__main__":
    asyncio.run(main())