#!/usr/bin/env python3
"""
HankookTire SmartSensor 2.0 - AI-Powered Anomaly Detection System
ì°¨ì„¸ëŒ€ AI ê¸°ë°˜ ì´ìƒ íƒì§€ ë° ì˜ˆì¸¡ ë¶„ì„ ì‹œìŠ¤í…œ

Advanced anomaly detection using multiple ML algorithms:
- Isolation Forest for outlier detection
- LSTM for time series anomaly detection
- Statistical analysis for sensor data validation
- Real-time prediction and alerting
"""

import asyncio
import numpy as np
import pandas as pd
import logging
import json
import os
import pickle
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

# ML/AI Libraries
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import DBSCAN
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import joblib

# Data Processing
import psycopg2
from psycopg2.extras import RealDictCursor
import redis
import aiohttp
from scipy import stats
from scipy.signal import savgol_filter

# Configuration
DATABASE_CONFIG = {
    'host': os.getenv('POSTGRES_HOST', 'postgres-service'),
    'port': int(os.getenv('POSTGRES_PORT', '5432')),
    'database': os.getenv('POSTGRES_DB', 'smarttire_sensors'),
    'user': os.getenv('POSTGRES_USER', 'smarttire'),
    'password': os.getenv('POSTGRES_PASSWORD', 'password')
}

REDIS_CONFIG = {
    'host': os.getenv('REDIS_HOST', 'redis-service'),
    'port': int(os.getenv('REDIS_PORT', '6379')),
    'password': os.getenv('REDIS_PASSWORD', '')
}

API_BASE_URL = os.getenv('API_BASE_URL', 'http://api-service:8000')
MODEL_PATH = os.getenv('MODEL_PATH', '/app/models')
ALERT_WEBHOOK = os.getenv('ALERT_WEBHOOK', '')

# Logging Setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/var/log/anomaly-detector.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AnomalyType(Enum):
    """ì´ìƒ íƒì§€ ìœ í˜•"""
    SENSOR_MALFUNCTION = "sensor_malfunction"
    TEMPERATURE_ANOMALY = "temperature_anomaly"
    PRESSURE_ANOMALY = "pressure_anomaly"
    BATTERY_DEGRADATION = "battery_degradation"
    COMMUNICATION_ISSUE = "communication_issue"
    DATA_QUALITY_DROP = "data_quality_drop"
    PREDICTIVE_MAINTENANCE = "predictive_maintenance"
    SECURITY_BREACH = "security_breach"

class SeverityLevel(Enum):
    """ì‹¬ê°ë„ ë ˆë²¨"""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4
    EMERGENCY = 5

@dataclass
class AnomalyResult:
    """ì´ìƒ íƒì§€ ê²°ê³¼"""
    device_id: str
    anomaly_type: AnomalyType
    severity: SeverityLevel
    confidence_score: float
    predicted_value: float
    actual_value: float
    threshold: float
    timestamp: datetime
    description: str
    recommendation: str
    model_used: str
    feature_importance: Dict[str, float] = None
    location: str = None
    maintenance_window: int = None  # ì˜ˆìƒ ìœ ì§€ë³´ìˆ˜ í•„ìš” ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„)

class AdvancedAnomalyDetector:
    """ê³ ê¸‰ AI ê¸°ë°˜ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.redis_client = None
        self.session = None
        self.model_metrics = {}
        
        # ëª¨ë¸ ì„¤ì •
        self.isolation_forest_params = {
            'contamination': 0.1,
            'random_state': 42,
            'n_estimators': 100
        }
        
        self.lstm_params = {
            'sequence_length': 60,  # 1ì‹œê°„ ë°ì´í„° (1ë¶„ ê°„ê²©)
            'features': ['temperature', 'humidity', 'pressure', 'battery_voltage', 'signal_strength'],
            'epochs': 100,
            'batch_size': 32
        }
        
        self.dbscan_params = {
            'eps': 0.5,
            'min_samples': 5
        }

    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì ì§„ì…"""
        # Redis ì—°ê²°
        self.redis_client = redis.Redis(
            host=REDIS_CONFIG['host'],
            port=REDIS_CONFIG['port'],
            password=REDIS_CONFIG['password'] if REDIS_CONFIG['password'] else None,
            decode_responses=True
        )
        
        # HTTP ì„¸ì…˜
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(timeout=timeout)
        
        # ëª¨ë¸ ë¡œë“œ
        await self.load_models()
        
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì ì¢…ë£Œ"""
        if self.session:
            await self.session.close()

    async def load_models(self):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        logger.info("ğŸ¤– AI ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        try:
            # Isolation Forest ëª¨ë¸
            isolation_forest_path = f"{MODEL_PATH}/isolation_forest.joblib"
            if os.path.exists(isolation_forest_path):
                self.models['isolation_forest'] = joblib.load(isolation_forest_path)
                logger.info("âœ… Isolation Forest ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                logger.info("ğŸ”„ Isolation Forest ëª¨ë¸ ì—†ìŒ - ìƒˆë¡œ í›ˆë ¨í•©ë‹ˆë‹¤")
                await self.train_isolation_forest()
            
            # LSTM ëª¨ë¸
            lstm_model_path = f"{MODEL_PATH}/lstm_anomaly_detector.h5"
            if os.path.exists(lstm_model_path):
                self.models['lstm'] = load_model(lstm_model_path)
                logger.info("âœ… LSTM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                logger.info("ğŸ”„ LSTM ëª¨ë¸ ì—†ìŒ - ìƒˆë¡œ í›ˆë ¨í•©ë‹ˆë‹¤")
                await self.train_lstm_model()
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            scaler_path = f"{MODEL_PATH}/scaler.joblib"
            if os.path.exists(scaler_path):
                self.scalers['standard'] = joblib.load(scaler_path)
                logger.info("âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")
            else:
                self.scalers['standard'] = StandardScaler()
                logger.info("ğŸ”„ ìƒˆ ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„±")
                
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            # ê¸°ë³¸ ëª¨ë¸ ìƒì„±
            await self.create_default_models()

    async def create_default_models(self):
        """ê¸°ë³¸ ëª¨ë¸ ìƒì„±"""
        logger.info("ğŸ”§ ê¸°ë³¸ ëª¨ë¸ ìƒì„± ì¤‘...")
        
        # ê¸°ë³¸ Isolation Forest
        self.models['isolation_forest'] = IsolationForest(**self.isolation_forest_params)
        
        # ê¸°ë³¸ LSTM ëª¨ë¸
        self.models['lstm'] = self.build_lstm_model()
        
        # ê¸°ë³¸ ìŠ¤ì¼€ì¼ëŸ¬
        self.scalers['standard'] = StandardScaler()
        self.scalers['minmax'] = MinMaxScaler()

    def build_lstm_model(self, input_shape=(60, 5)) -> tf.keras.Model:
        """LSTM ëª¨ë¸ êµ¬ì¡° ì •ì˜"""
        model = Sequential([
            Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape),
            Dropout(0.2),
            Bidirectional(LSTM(32, return_sequences=True)),
            Dropout(0.2),
            LSTM(16),
            Dropout(0.1),
            Dense(8, activation='relu'),
            Dense(1, activation='linear')  # ì´ìƒ ìŠ¤ì½”ì–´ ì¶œë ¥
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        return model

    async def get_sensor_data(self, hours_back: int = 24) -> pd.DataFrame:
        """ì„¼ì„œ ë°ì´í„° ì¡°íšŒ"""
        try:
            conn = psycopg2.connect(**DATABASE_CONFIG)
            
            query = """
            SELECT 
                sr.device_id,
                sr.timestamp,
                sr.temperature,
                sr.humidity,
                sr.pressure,
                sr.battery_voltage,
                sr.signal_strength,
                sr.acceleration_x,
                sr.acceleration_y,
                sr.acceleration_z,
                sr.quality_score,
                d.location_info,
                d.device_type,
                d.firmware_version
            FROM sensors.sensor_readings sr
            JOIN sensors.devices d ON sr.device_id = d.device_id
            WHERE sr.timestamp >= NOW() - INTERVAL %s
            ORDER BY sr.device_id, sr.timestamp
            """
            
            df = pd.read_sql_query(query, conn, params=(f'{hours_back} hours',))
            conn.close()
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.sort_values(['device_id', 'timestamp'])
            
            # íŒŒìƒ ë³€ìˆ˜ ìƒì„±
            df['acceleration_magnitude'] = np.sqrt(
                df['acceleration_x']**2 + 
                df['acceleration_y']**2 + 
                df['acceleration_z']**2
            )
            
            # ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±
            df['hour'] = df['timestamp'].dt.hour
            df['day_of_week'] = df['timestamp'].dt.dayofweek
            
            logger.info(f"ğŸ“Š ì„¼ì„œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)} ë ˆì½”ë“œ")
            return df
            
        except Exception as e:
            logger.error(f"âŒ ì„¼ì„œ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return pd.DataFrame()

    async def train_isolation_forest(self):
        """Isolation Forest ëª¨ë¸ í›ˆë ¨"""
        logger.info("ğŸ¯ Isolation Forest ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        try:
            # í›ˆë ¨ ë°ì´í„° ë¡œë“œ (ìµœê·¼ 7ì¼ê°„)
            df = await self.get_sensor_data(hours_back=168)
            
            if df.empty:
                logger.warning("âš ï¸ í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # íŠ¹ì„± ì„ íƒ
            features = ['temperature', 'humidity', 'pressure', 'battery_voltage', 
                       'signal_strength', 'acceleration_magnitude', 'quality_score']
            
            # ê²°ì¸¡ê°’ ì²˜ë¦¬
            df_clean = df[features].fillna(df[features].median())
            
            # ì´ìƒê°’ ì œê±° (3-sigma rule)
            df_clean = df_clean[(np.abs(stats.zscore(df_clean)) < 3).all(axis=1)]
            
            # ìŠ¤ì¼€ì¼ë§
            X_scaled = self.scalers['standard'].fit_transform(df_clean)
            
            # ëª¨ë¸ í›ˆë ¨
            self.models['isolation_forest'].fit(X_scaled)
            
            # ëª¨ë¸ ì €ì¥
            os.makedirs(MODEL_PATH, exist_ok=True)
            joblib.dump(self.models['isolation_forest'], f"{MODEL_PATH}/isolation_forest.joblib")
            joblib.dump(self.scalers['standard'], f"{MODEL_PATH}/scaler.joblib")
            
            # ì„±ëŠ¥ í‰ê°€
            anomaly_scores = self.models['isolation_forest'].decision_function(X_scaled)
            threshold = np.percentile(anomaly_scores, 10)  # í•˜ìœ„ 10%ë¥¼ ì´ìƒìœ¼ë¡œ ê°„ì£¼
            
            self.model_metrics['isolation_forest'] = {
                'threshold': threshold,
                'training_samples': len(X_scaled),
                'contamination_rate': self.isolation_forest_params['contamination'],
                'last_trained': datetime.now().isoformat()
            }
            
            logger.info(f"âœ… Isolation Forest í›ˆë ¨ ì™„ë£Œ - ìƒ˜í”Œ: {len(X_scaled)}, ì„ê³„ê°’: {threshold:.4f}")
            
        except Exception as e:
            logger.error(f"âŒ Isolation Forest í›ˆë ¨ ì‹¤íŒ¨: {str(e)}")

    async def train_lstm_model(self):
        """LSTM ì‹œê³„ì—´ ì´ìƒ íƒì§€ ëª¨ë¸ í›ˆë ¨"""
        logger.info("ğŸ§  LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        try:
            # í›ˆë ¨ ë°ì´í„° ë¡œë“œ (ìµœê·¼ 30ì¼ê°„)
            df = await self.get_sensor_data(hours_back=720)
            
            if len(df) < 1000:
                logger.warning("âš ï¸ LSTM í›ˆë ¨ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # ë””ë°”ì´ìŠ¤ë³„ë¡œ ì‹œí€€ìŠ¤ ìƒì„±
            sequences = []
            targets = []
            
            for device_id in df['device_id'].unique():
                device_data = df[df['device_id'] == device_id].sort_values('timestamp')
                
                if len(device_data) < self.lstm_params['sequence_length']:
                    continue
                
                # íŠ¹ì„± ì„ íƒ ë° ì •ê·œí™”
                features = device_data[self.lstm_params['features']].fillna(method='ffill')
                features_scaled = self.scalers['minmax'].fit_transform(features)
                
                # ì‹œí€€ìŠ¤ ìƒì„±
                for i in range(len(features_scaled) - self.lstm_params['sequence_length']):
                    sequences.append(features_scaled[i:i+self.lstm_params['sequence_length']])
                    
                    # ë‹¤ìŒ ê°’ ì˜ˆì¸¡ì„ ìœ„í•œ íƒ€ê²Ÿ (ì˜¨ë„ ê¸°ì¤€)
                    targets.append(features_scaled[i+self.lstm_params['sequence_length'], 0])
            
            if len(sequences) == 0:
                logger.warning("âš ï¸ ìƒì„±ëœ ì‹œí€€ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            X = np.array(sequences)
            y = np.array(targets)
            
            # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• 
            split_idx = int(len(X) * 0.8)
            X_train, X_val = X[:split_idx], X[split_idx:]
            y_train, y_val = y[:split_idx], y[split_idx:]
            
            # ëª¨ë¸ í›ˆë ¨
            callbacks = [
                EarlyStopping(patience=10, restore_best_weights=True),
                ModelCheckpoint(f"{MODEL_PATH}/lstm_anomaly_detector.h5", save_best_only=True)
            ]
            
            history = self.models['lstm'].fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=self.lstm_params['epochs'],
                batch_size=self.lstm_params['batch_size'],
                callbacks=callbacks,
                verbose=1
            )
            
            # ì„±ëŠ¥ í‰ê°€
            val_loss = min(history.history['val_loss'])
            val_mae = min(history.history['val_mae'])
            
            self.model_metrics['lstm'] = {
                'val_loss': val_loss,
                'val_mae': val_mae,
                'training_samples': len(X_train),
                'validation_samples': len(X_val),
                'epochs_trained': len(history.history['loss']),
                'last_trained': datetime.now().isoformat()
            }
            
            logger.info(f"âœ… LSTM ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ - Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}")
            
        except Exception as e:
            logger.error(f"âŒ LSTM ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}")

    async def detect_isolation_forest_anomalies(self, df: pd.DataFrame) -> List[AnomalyResult]:
        """Isolation Forestë¥¼ ì´ìš©í•œ ì´ìƒ íƒì§€"""
        results = []
        
        if 'isolation_forest' not in self.models:
            return results
        
        try:
            features = ['temperature', 'humidity', 'pressure', 'battery_voltage', 
                       'signal_strength', 'acceleration_magnitude', 'quality_score']
            
            for device_id in df['device_id'].unique():
                device_data = df[df['device_id'] == device_id].copy()
                
                if len(device_data) < 5:  # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­
                    continue
                
                # íŠ¹ì„± ì¶”ì¶œ ë° ì „ì²˜ë¦¬
                X = device_data[features].fillna(device_data[features].median())
                X_scaled = self.scalers['standard'].transform(X)
                
                # ì´ìƒ íƒì§€
                anomaly_scores = self.models['isolation_forest'].decision_function(X_scaled)
                anomaly_predictions = self.models['isolation_forest'].predict(X_scaled)
                
                # ì´ìƒì¹˜ ë¶„ì„
                anomaly_indices = np.where(anomaly_predictions == -1)[0]
                
                for idx in anomaly_indices:
                    row = device_data.iloc[idx]
                    score = anomaly_scores[idx]
                    
                    # ì‹¬ê°ë„ í‰ê°€
                    if score < -0.5:
                        severity = SeverityLevel.CRITICAL
                    elif score < -0.3:
                        severity = SeverityLevel.HIGH
                    elif score < -0.1:
                        severity = SeverityLevel.MEDIUM
                    else:
                        severity = SeverityLevel.LOW
                    
                    # ì´ìƒ ìœ í˜• ë¶„ë¥˜
                    anomaly_type = self.classify_anomaly_type(row, features, X.iloc[idx])
                    
                    # íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°
                    feature_importance = dict(zip(features, np.abs(X_scaled[idx])))
                    
                    results.append(AnomalyResult(
                        device_id=device_id,
                        anomaly_type=anomaly_type,
                        severity=severity,
                        confidence_score=abs(score),
                        predicted_value=0.0,  # Isolation ForestëŠ” ì˜ˆì¸¡ê°’ ì—†ìŒ
                        actual_value=score,
                        threshold=self.model_metrics.get('isolation_forest', {}).get('threshold', -0.1),
                        timestamp=row['timestamp'],
                        description=f"Isolation Forestì—ì„œ ì´ìƒ íŒ¨í„´ ê°ì§€ (ì ìˆ˜: {score:.4f})",
                        recommendation=self.get_recommendation(anomaly_type, severity),
                        model_used="Isolation Forest",
                        feature_importance=feature_importance,
                        location=row.get('location_info', 'Unknown')
                    ))
            
            logger.info(f"ğŸ” Isolation Forest ì´ìƒ íƒì§€ ì™„ë£Œ: {len(results)}ê°œ ë°œê²¬")
            
        except Exception as e:
            logger.error(f"âŒ Isolation Forest ì´ìƒ íƒì§€ ì‹¤íŒ¨: {str(e)}")
        
        return results

    async def detect_lstm_anomalies(self, df: pd.DataFrame) -> List[AnomalyResult]:
        """LSTM ì‹œê³„ì—´ ì´ìƒ íƒì§€"""
        results = []
        
        if 'lstm' not in self.models:
            return results
        
        try:
            for device_id in df['device_id'].unique():
                device_data = df[df['device_id'] == device_id].sort_values('timestamp')
                
                if len(device_data) < self.lstm_params['sequence_length']:
                    continue
                
                # íŠ¹ì„± ì¤€ë¹„
                features = device_data[self.lstm_params['features']].fillna(method='ffill')
                features_scaled = self.scalers['minmax'].transform(features)
                
                # ì‹œí€€ìŠ¤ ìƒì„± ë° ì˜ˆì¸¡
                for i in range(len(features_scaled) - self.lstm_params['sequence_length']):
                    sequence = features_scaled[i:i+self.lstm_params['sequence_length']]
                    X_seq = sequence.reshape(1, self.lstm_params['sequence_length'], len(self.lstm_params['features']))
                    
                    # ì˜ˆì¸¡
                    predicted = self.models['lstm'].predict(X_seq, verbose=0)[0][0]
                    actual = features_scaled[i+self.lstm_params['sequence_length'], 0]  # ì˜¨ë„ ê¸°ì¤€
                    
                    # ì˜ˆì¸¡ ì˜¤ì°¨ ê³„ì‚°
                    error = abs(predicted - actual)
                    
                    # ì´ìƒ íƒì§€ (ì„ê³„ê°’ ê¸°ë°˜)
                    threshold = 0.1  # ì •ê·œí™”ëœ ë°ì´í„° ê¸°ì¤€
                    
                    if error > threshold:
                        row = device_data.iloc[i+self.lstm_params['sequence_length']]
                        
                        # ì‹¬ê°ë„ í‰ê°€
                        if error > 0.3:
                            severity = SeverityLevel.CRITICAL
                        elif error > 0.2:
                            severity = SeverityLevel.HIGH
                        elif error > 0.15:
                            severity = SeverityLevel.MEDIUM
                        else:
                            severity = SeverityLevel.LOW
                        
                        results.append(AnomalyResult(
                            device_id=device_id,
                            anomaly_type=AnomalyType.TEMPERATURE_ANOMALY,
                            severity=severity,
                            confidence_score=min(error * 10, 1.0),  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
                            predicted_value=predicted,
                            actual_value=actual,
                            threshold=threshold,
                            timestamp=row['timestamp'],
                            description=f"LSTM ì‹œê³„ì—´ ì˜ˆì¸¡ì—ì„œ í° ì˜¤ì°¨ ê°ì§€ (ì˜¤ì°¨: {error:.4f})",
                            recommendation=self.get_recommendation(AnomalyType.TEMPERATURE_ANOMALY, severity),
                            model_used="LSTM",
                            location=row.get('location_info', 'Unknown')
                        ))
            
            logger.info(f"ğŸ§  LSTM ì´ìƒ íƒì§€ ì™„ë£Œ: {len(results)}ê°œ ë°œê²¬")
            
        except Exception as e:
            logger.error(f"âŒ LSTM ì´ìƒ íƒì§€ ì‹¤íŒ¨: {str(e)}")
        
        return results

    def classify_anomaly_type(self, row: pd.Series, features: List[str], feature_values: pd.Series) -> AnomalyType:
        """ì´ìƒ ìœ í˜• ë¶„ë¥˜"""
        # ì˜¨ë„ ì´ìƒ
        if 'temperature' in features and abs(feature_values['temperature']) > 2:
            return AnomalyType.TEMPERATURE_ANOMALY
        
        # ì••ë ¥ ì´ìƒ
        if 'pressure' in features and abs(feature_values['pressure']) > 2:
            return AnomalyType.PRESSURE_ANOMALY
        
        # ë°°í„°ë¦¬ ì´ìƒ
        if 'battery_voltage' in features and feature_values['battery_voltage'] < -1:
            return AnomalyType.BATTERY_DEGRADATION
        
        # í†µì‹  ì´ìƒ
        if 'signal_strength' in features and feature_values['signal_strength'] < -1:
            return AnomalyType.COMMUNICATION_ISSUE
        
        # ë°ì´í„° í’ˆì§ˆ ì´ìƒ
        if 'quality_score' in features and feature_values['quality_score'] < -1:
            return AnomalyType.DATA_QUALITY_DROP
        
        # ê¸°ë³¸ê°’
        return AnomalyType.SENSOR_MALFUNCTION

    def get_recommendation(self, anomaly_type: AnomalyType, severity: SeverityLevel) -> str:
        """ì´ìƒ ìœ í˜•ë³„ ê¶Œì¥ì‚¬í•­"""
        recommendations = {
            AnomalyType.SENSOR_MALFUNCTION: {
                SeverityLevel.LOW: "ì„¼ì„œ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ê°•í™”",
                SeverityLevel.MEDIUM: "ì„¼ì„œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê³ ë ¤",
                SeverityLevel.HIGH: "ì„¼ì„œ ì ê²€ ë° ì²­ì†Œ í•„ìš”",
                SeverityLevel.CRITICAL: "ì¦‰ì‹œ ì„¼ì„œ êµì²´ ê²€í† ",
                SeverityLevel.EMERGENCY: "ê¸´ê¸‰ ì„¼ì„œ êµì²´ í•„ìš”"
            },
            AnomalyType.TEMPERATURE_ANOMALY: {
                SeverityLevel.LOW: "ì˜¨ë„ ë³€í™” ì¶”ì´ ê´€ì°°",
                SeverityLevel.MEDIUM: "í™˜ê²½ ìš”ì¸ í™•ì¸",
                SeverityLevel.HIGH: "ëƒ‰ê° ì‹œìŠ¤í…œ ì ê²€",
                SeverityLevel.CRITICAL: "ì¦‰ì‹œ ì˜¨ë„ ì¡°ì¹˜ í•„ìš”",
                SeverityLevel.EMERGENCY: "ê¸´ê¸‰ ì•ˆì „ ì¡°ì¹˜ ì‹¤í–‰"
            },
            AnomalyType.PRESSURE_ANOMALY: {
                SeverityLevel.LOW: "ì••ë ¥ ë³€í™” ëª¨ë‹ˆí„°ë§",
                SeverityLevel.MEDIUM: "ì••ë ¥ ì„¼ì„œ ì ê²€",
                SeverityLevel.HIGH: "ì••ë ¥ ì‹œìŠ¤í…œ ì§„ë‹¨",
                SeverityLevel.CRITICAL: "ì¦‰ì‹œ ì••ë ¥ ì¡°ì • í•„ìš”",
                SeverityLevel.EMERGENCY: "ì•ˆì „ì„ ìœ„í•œ ì¦‰ì‹œ ì •ì§€"
            },
            AnomalyType.BATTERY_DEGRADATION: {
                SeverityLevel.LOW: "ë°°í„°ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§",
                SeverityLevel.MEDIUM: "ë°°í„°ë¦¬ ìµœì í™” ì„¤ì •",
                SeverityLevel.HIGH: "ë°°í„°ë¦¬ êµì²´ ì¤€ë¹„",
                SeverityLevel.CRITICAL: "ì¦‰ì‹œ ë°°í„°ë¦¬ êµì²´",
                SeverityLevel.EMERGENCY: "ê¸´ê¸‰ ì „ì› ê³µê¸‰ í•„ìš”"
            },
            AnomalyType.COMMUNICATION_ISSUE: {
                SeverityLevel.LOW: "ì‹ í˜¸ ê°•ë„ ëª¨ë‹ˆí„°ë§",
                SeverityLevel.MEDIUM: "ë„¤íŠ¸ì›Œí¬ ì„¤ì • í™•ì¸",
                SeverityLevel.HIGH: "í†µì‹  ì¥ë¹„ ì ê²€",
                SeverityLevel.CRITICAL: "ë„¤íŠ¸ì›Œí¬ ì¸í”„ë¼ ì§„ë‹¨",
                SeverityLevel.EMERGENCY: "ë°±ì—… í†µì‹  ìˆ˜ë‹¨ í™œì„±í™”"
            },
            AnomalyType.DATA_QUALITY_DROP: {
                SeverityLevel.LOW: "ë°ì´í„° ê²€ì¦ ê°•í™”",
                SeverityLevel.MEDIUM: "ì„¼ì„œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜",
                SeverityLevel.HIGH: "ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì ê²€",
                SeverityLevel.CRITICAL: "ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ ì¬ì‹œì‘",
                SeverityLevel.EMERGENCY: "ìˆ˜ë™ ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“œ"
            }
        }
        
        return recommendations.get(anomaly_type, {}).get(
            severity, 
            "ì „ë¬¸ê°€ ê²€í†  í•„ìš”"
        )

    async def predict_maintenance_needs(self, df: pd.DataFrame) -> List[AnomalyResult]:
        """ì˜ˆì¸¡ ìœ ì§€ë³´ìˆ˜ ë¶„ì„"""
        results = []
        
        try:
            for device_id in df['device_id'].unique():
                device_data = df[df['device_id'] == device_id].sort_values('timestamp')
                
                if len(device_data) < 24:  # ìµœì†Œ 24ì‹œê°„ ë°ì´í„°
                    continue
                
                # ë°°í„°ë¦¬ ìˆ˜ëª… ì˜ˆì¸¡
                battery_data = device_data['battery_voltage'].dropna()
                if len(battery_data) > 10:
                    # ë°°í„°ë¦¬ ë°©ì „ ì¶”ì„¸ ë¶„ì„
                    x = np.arange(len(battery_data))
                    slope, intercept, r_value, p_value, std_err = stats.linregress(x, battery_data)
                    
                    if slope < -0.001 and r_value < -0.5:  # ê°ì†Œ ì¶”ì„¸
                        # ì„ê³„ ì „ì•• (3.0V)ê¹Œì§€ ë‚¨ì€ ì‹œê°„ ì˜ˆì¸¡
                        current_voltage = battery_data.iloc[-1]
                        if current_voltage > 3.0:
                            hours_to_critical = (current_voltage - 3.0) / abs(slope)
                            
                            if hours_to_critical < 168:  # 1ì£¼ì¼ ë¯¸ë§Œ
                                severity = SeverityLevel.HIGH if hours_to_critical < 48 else SeverityLevel.MEDIUM
                                
                                results.append(AnomalyResult(
                                    device_id=device_id,
                                    anomaly_type=AnomalyType.PREDICTIVE_MAINTENANCE,
                                    severity=severity,
                                    confidence_score=abs(r_value),
                                    predicted_value=3.0,
                                    actual_value=current_voltage,
                                    threshold=3.2,
                                    timestamp=device_data['timestamp'].iloc[-1],
                                    description=f"ë°°í„°ë¦¬ êµì²´ í•„ìš” ì˜ˆìƒ: {hours_to_critical:.0f}ì‹œê°„ í›„",
                                    recommendation=f"ì˜ˆë°©ì  ë°°í„°ë¦¬ êµì²´ ê³„íš ìˆ˜ë¦½ ({hours_to_critical:.0f}ì‹œê°„ ë‚´)",
                                    model_used="Linear Regression",
                                    location=device_data['location_info'].iloc[-1] if 'location_info' in device_data.columns else 'Unknown',
                                    maintenance_window=int(hours_to_critical)
                                ))
                
                # ì„¼ì„œ í’ˆì§ˆ ì €í•˜ ì˜ˆì¸¡
                quality_data = device_data['quality_score'].dropna()
                if len(quality_data) > 10:
                    # ìŠ¤ë¬´ë”©ì„ í†µí•œ ì¶”ì„¸ ë¶„ì„
                    if len(quality_data) > 20:
                        smoothed = savgol_filter(quality_data, window_length=min(11, len(quality_data)//2*2+1), polyorder=2)
                        recent_trend = np.mean(np.diff(smoothed[-10:]))
                        
                        if recent_trend < -0.5:  # í’ˆì§ˆ ì €í•˜ ì¶”ì„¸
                            results.append(AnomalyResult(
                                device_id=device_id,
                                anomaly_type=AnomalyType.PREDICTIVE_MAINTENANCE,
                                severity=SeverityLevel.MEDIUM,
                                confidence_score=0.7,
                                predicted_value=quality_data.iloc[-1] + recent_trend * 24,
                                actual_value=quality_data.iloc[-1],
                                threshold=70.0,
                                timestamp=device_data['timestamp'].iloc[-1],
                                description=f"ì„¼ì„œ í’ˆì§ˆ ì €í•˜ ì¶”ì„¸ ê°ì§€ (ì¶”ì„¸: {recent_trend:.2f}/ì‹œê°„)",
                                recommendation="ì„¼ì„œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë° ì²­ì†Œ ê³„íš",
                                model_used="Trend Analysis",
                                location=device_data['location_info'].iloc[-1] if 'location_info' in device_data.columns else 'Unknown',
                                maintenance_window=48
                            ))
            
            logger.info(f"ğŸ”® ì˜ˆì¸¡ ìœ ì§€ë³´ìˆ˜ ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ ì˜ˆì¸¡")
            
        except Exception as e:
            logger.error(f"âŒ ì˜ˆì¸¡ ìœ ì§€ë³´ìˆ˜ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        return results

    async def detect_security_anomalies(self, df: pd.DataFrame) -> List[AnomalyResult]:
        """ë³´ì•ˆ ì´ìƒ íƒì§€"""
        results = []
        
        try:
            # ë¹„ì •ìƒì ì¸ ì ‘ì† íŒ¨í„´ ë¶„ì„
            device_counts = df.groupby(['device_id', df['timestamp'].dt.hour]).size().reset_index(name='count')
            
            for device_id in df['device_id'].unique():
                device_hourly = device_counts[device_counts['device_id'] == device_id]
                
                if len(device_hourly) > 0:
                    # DBSCANì„ ì´ìš©í•œ ì´ìƒ íŒ¨í„´ íƒì§€
                    X = device_hourly[['timestamp', 'count']].values
                    
                    if len(X) > 5:
                        clustering = DBSCAN(**self.dbscan_params).fit(X)
                        anomaly_mask = clustering.labels_ == -1
                        
                        if anomaly_mask.any():
                            anomaly_hours = device_hourly[anomaly_mask]
                            
                            for _, row in anomaly_hours.iterrows():
                                # ìµœê·¼ ë°ì´í„°ì—ì„œ í•´ë‹¹ ì‹œê°„ëŒ€ ì°¾ê¸°
                                timestamp = df[
                                    (df['device_id'] == device_id) & 
                                    (df['timestamp'].dt.hour == row['timestamp'])
                                ]['timestamp'].max()
                                
                                results.append(AnomalyResult(
                                    device_id=device_id,
                                    anomaly_type=AnomalyType.SECURITY_BREACH,
                                    severity=SeverityLevel.HIGH,
                                    confidence_score=0.8,
                                    predicted_value=0,
                                    actual_value=row['count'],
                                    threshold=0,
                                    timestamp=timestamp,
                                    description=f"ë¹„ì •ìƒì ì¸ ì ‘ì† íŒ¨í„´ ê°ì§€ (ì‹œê°„ëŒ€: {row['timestamp']}ì‹œ, íšŸìˆ˜: {row['count']})",
                                    recommendation="ë³´ì•ˆ ì ê²€ ë° ì ‘ì† ë¡œê·¸ ë¶„ì„ í•„ìš”",
                                    model_used="DBSCAN Clustering"
                                ))
            
            logger.info(f"ğŸ”’ ë³´ì•ˆ ì´ìƒ íƒì§€ ì™„ë£Œ: {len(results)}ê°œ ë°œê²¬")
            
        except Exception as e:
            logger.error(f"âŒ ë³´ì•ˆ ì´ìƒ íƒì§€ ì‹¤íŒ¨: {str(e)}")
        
        return results

    async def run_comprehensive_analysis(self) -> List[AnomalyResult]:
        """ì¢…í•© ì´ìƒ íƒì§€ ë¶„ì„"""
        logger.info("ğŸ” ì¢…í•© AI ì´ìƒ íƒì§€ ë¶„ì„ ì‹œì‘...")
        
        all_results = []
        
        try:
            # ìµœê·¼ ë°ì´í„° ë¡œë“œ
            df = await self.get_sensor_data(hours_back=24)
            
            if df.empty:
                logger.warning("âš ï¸ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return all_results
            
            # ê° ì•Œê³ ë¦¬ì¦˜ë³„ ì´ìƒ íƒì§€ ì‹¤í–‰
            detection_tasks = [
                self.detect_isolation_forest_anomalies(df),
                self.detect_lstm_anomalies(df),
                self.predict_maintenance_needs(df),
                self.detect_security_anomalies(df)
            ]
            
            results_list = await asyncio.gather(*detection_tasks, return_exceptions=True)
            
            # ê²°ê³¼ í†µí•©
            for results in results_list:
                if isinstance(results, list):
                    all_results.extend(results)
                elif isinstance(results, Exception):
                    logger.error(f"âŒ ì´ìƒ íƒì§€ ì‘ì—… ì‹¤íŒ¨: {str(results)}")
            
            # ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
            all_results = self.deduplicate_and_prioritize(all_results)
            
            # ê²°ê³¼ ì €ì¥
            await self.save_anomaly_results(all_results)
            
            # ì•Œë¦¼ ì „ì†¡
            await self.send_anomaly_alerts(all_results)
            
            logger.info(f"âœ… ì¢…í•© ì´ìƒ íƒì§€ ì™„ë£Œ: ì´ {len(all_results)}ê°œ ì´ìƒ ê°ì§€")
            
        except Exception as e:
            logger.error(f"âŒ ì¢…í•© ì´ìƒ íƒì§€ ì‹¤íŒ¨: {str(e)}")
        
        return all_results

    def deduplicate_and_prioritize(self, results: List[AnomalyResult]) -> List[AnomalyResult]:
        """ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬"""
        # ë””ë°”ì´ìŠ¤ë³„, ìœ í˜•ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¤‘ë³µ ì œê±°
        unique_results = {}
        
        for result in results:
            key = f"{result.device_id}_{result.anomaly_type.value}"
            
            if key not in unique_results or result.severity.value > unique_results[key].severity.value:
                unique_results[key] = result
        
        # ì‹¬ê°ë„ ë° ì‹ ë¢°ë„ë¡œ ì •ë ¬
        sorted_results = sorted(
            unique_results.values(),
            key=lambda x: (x.severity.value, x.confidence_score),
            reverse=True
        )
        
        return sorted_results

    async def save_anomaly_results(self, results: List[AnomalyResult]):
        """ì´ìƒ íƒì§€ ê²°ê³¼ ì €ì¥"""
        if not results:
            return
        
        try:
            conn = psycopg2.connect(**DATABASE_CONFIG)
            cursor = conn.cursor()
            
            for result in results:
                cursor.execute("""
                INSERT INTO analytics.anomalies 
                (device_id, timestamp, anomaly_type, severity, anomaly_score, 
                 affected_sensors, description, resolved, created_at)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    result.device_id,
                    result.timestamp,
                    result.anomaly_type.value,
                    result.severity.name,
                    result.confidence_score,
                    [result.model_used],
                    result.description,
                    False,
                    datetime.now()
                ))
            
            conn.commit()
            cursor.close()
            conn.close()
            
            logger.info(f"ğŸ’¾ ì´ìƒ íƒì§€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(results)}ê±´")
            
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    async def send_anomaly_alerts(self, results: List[AnomalyResult]):
        """ì´ìƒ íƒì§€ ì•Œë¦¼ ì „ì†¡"""
        if not results or not ALERT_WEBHOOK:
            return
        
        try:
            # ì‹¬ê°ë„ë³„ í•„í„°ë§
            critical_results = [r for r in results if r.severity.value >= SeverityLevel.HIGH.value]
            
            if not critical_results:
                return
            
            # ì•Œë¦¼ ë©”ì‹œì§€ êµ¬ì„±
            alert_data = {
                "timestamp": datetime.now().isoformat(),
                "total_anomalies": len(results),
                "critical_anomalies": len(critical_results),
                "anomalies": [
                    {
                        "device_id": result.device_id,
                        "type": result.anomaly_type.value,
                        "severity": result.severity.name,
                        "confidence": result.confidence_score,
                        "description": result.description,
                        "recommendation": result.recommendation,
                        "location": result.location,
                        "timestamp": result.timestamp.isoformat()
                    }
                    for result in critical_results[:10]  # ìµœëŒ€ 10ê°œë§Œ ì „ì†¡
                ]
            }
            
            async with self.session.post(ALERT_WEBHOOK, json=alert_data) as response:
                if response.status == 200:
                    logger.info(f"ğŸš¨ ì´ìƒ íƒì§€ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ: {len(critical_results)}ê±´")
                else:
                    logger.error(f"âŒ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: HTTP {response.status}")
                    
        except Exception as e:
            logger.error(f"âŒ ì•Œë¦¼ ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    async def retrain_models(self):
        """ëª¨ë¸ ì¬í›ˆë ¨ (ì •ê¸°ì  ì‹¤í–‰)"""
        logger.info("ğŸ”„ AI ëª¨ë¸ ì¬í›ˆë ¨ ì‹œì‘...")
        
        try:
            # ìƒˆë¡œìš´ í›ˆë ¨ ë°ì´í„°ë¡œ ëª¨ë¸ ì—…ë°ì´íŠ¸
            await self.train_isolation_forest()
            await self.train_lstm_model()
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            await self.update_model_metrics()
            
            logger.info("âœ… AI ëª¨ë¸ ì¬í›ˆë ¨ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì¬í›ˆë ¨ ì‹¤íŒ¨: {str(e)}")

    async def update_model_metrics(self):
        """ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        try:
            # Redisì— ë©”íŠ¸ë¦­ ì €ì¥
            metrics_key = "ai_model_metrics"
            metrics_data = {
                "last_updated": datetime.now().isoformat(),
                "models": self.model_metrics
            }
            
            self.redis_client.setex(
                metrics_key, 
                86400,  # 24ì‹œê°„ TTL
                json.dumps(metrics_data)
            )
            
            logger.info("ğŸ“Š ëª¨ë¸ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ HankookTire SmartSensor AI ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ ì‹œì‘")
    
    async with AdvancedAnomalyDetector() as detector:
        # ì¢…í•© ì´ìƒ íƒì§€ ì‹¤í–‰
        results = await detector.run_comprehensive_analysis()
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if results:
            logger.info("=" * 80)
            logger.info("ğŸ¤– AI ì´ìƒ íƒì§€ ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 80)
            
            severity_counts = {}
            type_counts = {}
            
            for result in results:
                severity_counts[result.severity.name] = severity_counts.get(result.severity.name, 0) + 1
                type_counts[result.anomaly_type.value] = type_counts.get(result.anomaly_type.value, 0) + 1
            
            logger.info(f"ğŸ“Š ì´ ì´ìƒ ê°ì§€: {len(results)}ê±´")
            logger.info(f"ğŸ“‹ ì‹¬ê°ë„ë³„: {severity_counts}")
            logger.info(f"ğŸ” ìœ í˜•ë³„: {type_counts}")
            
            # ìƒìœ„ 5ê°œ ì´ìƒ ì¶œë ¥
            logger.info("\nğŸš¨ ì£¼ìš” ì´ìƒ íƒì§€ ê²°ê³¼:")
            for i, result in enumerate(results[:5], 1):
                logger.info(f"{i}. {result.device_id} - {result.anomaly_type.value}")
                logger.info(f"   ì‹¬ê°ë„: {result.severity.name}, ì‹ ë¢°ë„: {result.confidence_score:.3f}")
                logger.info(f"   ì„¤ëª…: {result.description}")
                logger.info(f"   ê¶Œì¥ì‚¬í•­: {result.recommendation}")
                logger.info("")
        else:
            logger.info("âœ… ì´ìƒ ì§•í›„ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì •ê¸°ì  ëª¨ë¸ ì¬í›ˆë ¨ (ì£¼ê°„ ì‹¤í–‰ ì‹œ)
        if datetime.now().weekday() == 6:  # ì¼ìš”ì¼
            await detector.retrain_models()
        
        logger.info("ğŸ‰ AI ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ ì‹¤í–‰ ì™„ë£Œ")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ AI ì´ìƒ íƒì§€ ì‹œìŠ¤í…œì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        exit(1)